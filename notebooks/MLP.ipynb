{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Diagnostic MLP – 430 Features\n", "\n", "Version 0.2  \n", "Auteur : Yoan  \n", "Date : 2025‑06‑30\n", "\n", "Objectif : prédire automatiquement si un patient est malade (1) ou sain (0) à l’aide de 430 caractéristiques numériques extraites de données d’IRM.\n", "\n", "Le notebook suit un pipeline complet : ingestion, feature engineering, split, hyperparameter tuning, entraînement final, évaluation et explicabilité.\n"]}, {"cell_type": "code", "execution_count": null, "id": "0384e6e6", "metadata": {}, "outputs": [], "source": ["# --- Configuration générale ---\n", "from pathlib import Path\n", "from robust_evaluation_tools.robust_MLP import PatientMLP, MODEL_DIR, build_mlp_from_config, DEFAULT_MODEL_CONFIG\n", "DATA_DIR  = Path(\"DONNES_F/COMPILATIONS_AUG_3/\")      # <-- adapte si besoin\n", "disease = \"ALL\"\n", "RUN_NAME  = f\"mlp7_{disease}\"\n", "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n", "SEED = 41\n", "\n", "from robust_evaluation_tools.MLP_train import (\n", "    PatientDataset, make_loaders, train_epoch, eval_epoch, fit\n", ")\n"]}, {"cell_type": "markdown", "id": "6cd7b2ee", "metadata": {}, "source": ["## Guide d’utilisation & Configuration unique\n", "\n", "- Tous les paramètres du modèle et de l’entraînement sont centralisés ci‑dessous.\n", "- Pour changer le nombre de couches, modifiez `MODEL_CFG[\"hidden_dims\"]` (ex: `[512, 256]`).\n", "- Activez/désactivez Optuna via `USE_OPTUNA`.\n", "- Les artefacts sont enregistrés dans `Pytorch_models/` avec `RUN_NAME`.\n"]}, {"cell_type": "code", "execution_count": null, "id": "91072e5d", "metadata": {}, "outputs": [], "source": ["# --- Config centralisée (modèle + entraînement) ---\n", "from robust_evaluation_tools.robust_MLP import build_mlp_from_config, DEFAULT_MODEL_CONFIG\n", "\n", "USE_OPTUNA = False  # False: utilise MODEL_CFG; True: lance l’Optuna\n", "\n", "MODEL_CFG = {\n", "    **DEFAULT_MODEL_CONFIG,\n", "    # Modèle\n", "    \"in_features\": 430,              # change si le nb de features évolue\n", "    \"hidden_dims\": [256, 128, 64],   # nb de couches = len(hidden_dims)\n", "    \"activation\": \"relu\",           # \"relu\" | \"gelu\" | \"leaky_relu\" | \"elu\" | \"tanh\"\n", "    \"batch_norm\": True,\n", "    \"dropout\": 0.5,                  # float ou liste par couche, ex: [0.1, 0.2, 0.2]\n", "}\n", "\n", "TRAIN_CFG = {\n", "    \"batch_size\": 64,\n", "    \"lr\": 1e-3,\n", "    \"weight_decay\": 1e-4,\n", "    \"epochs\": 100,\n", "    \"patience\": 10,\n", "    \"neg_weight\": 10.0,              # pondération de la classe 0 dans la loss\n", "}\n", "\n", "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "print(\"Device:\", device)\n"]}, {"cell_type": "code", "execution_count": null, "id": "d95be5da", "metadata": {}, "outputs": [], "source": ["# INSTALLATION (décommente si nécessaire)\n", "# %pip install -q pandas numpy scikit-learn torch optuna shap tensorboard joblib tqdm\n"]}, {"cell_type": "code", "execution_count": null, "id": "68aa66e8", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import torch, torch.nn as nn\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import (classification_report, roc_auc_score, f1_score,\n", "                             confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay,\n", "                             PrecisionRecallDisplay)\n", "from torch.utils.data import Dataset, DataLoader\n", "from torch.utils.tensorboard import SummaryWriter\n", "import joblib, random, os, json, optuna\n", "from tqdm.auto import tqdm\n", "\n", "from robust_evaluation_tools.synthectic_sites_generations import augment_df, split_train_test, generate_sites_no_file\n", "from robust_evaluation_tools.robust_utils import remove_covariates_effects_metrics\n", "\n", "# ----- Helpers -----\n", "def set_seed(seed: int = 42):\n", "    np.random.seed(seed)\n", "    random.seed(seed)\n", "    torch.manual_seed(seed)\n", "    torch.cuda.manual_seed_all(seed)\n", "set_seed(SEED)\n", "\n", "device = \"cpu\"\n", "print(\"Device:\", device)\n", "\n", "def show_class_balance(y):\n", "    vals, counts = np.unique(y, return_counts=True)\n", "    for v, c in zip(vals, counts):\n", "        print(f\"Classe {int(v)} : {c}\")\n", "\n", "def plot_curves(train, val, ylabel=\"Loss\"):\n", "    plt.figure(figsize=(6,4))\n", "    epochs = range(1, len(train)+1)\n", "    plt.plot(epochs, train, label=\"train\")\n", "    plt.plot(epochs, val,   label=\"val\")\n", "    plt.xlabel(\"Epoch\")\n", "    plt.ylabel(ylabel)\n", "    plt.title(f\"Courbe {ylabel}\")\n", "    plt.legend(); plt.grid(True); plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "c5721f37", "metadata": {}, "outputs": [], "source": ["\n", "\n", "# Si disease == \"ALL\", on fusionne toutes les maladies sans doublons de SID\n", "if disease == \"ALL\":\n", "    # sids_vus = set()\n", "    # df_total = pd.DataFrame()\n", "    # for maladie in [\"AD\", \"ADHD\", \"BIP\", \"MCI\", \"SCHZ\", \"TBI\"]:\n", "    #     df_raw = pd.read_csv(DATA_DIR / f\"{maladie}_combination_all_metrics_CamCAN.csv.gz\")\n", "        \n", "    #     # On enlève les SIDs déjà vus\n", "    #     df_filtré = df_raw[~df_raw[\"sid\"].isin(sids_vus)]\n", "        \n", "    #     # On ajoute les nouveaux SIDs à notre set\n", "    #     sids_vus.update(df_filtré[\"sid\"].unique())\n", "        \n", "    #     # On concatène le DataFrame filtré\n", "    #     df_total = pd.concat([df_total, df_filtré], ignore_index=True)\n", "    # df_raw = df_total\n", "    df_raw = pd.read_csv(\"DONNES_MLP/train_data_all.csv\")\n", "    df_raw[~((df_raw['disease'] == 'HC') & (df_raw['old_site'] != 'CamCAN'))]\n", "else:\n", "    df_raw = pd.read_csv(DATA_DIR / f\"{disease}_combination_all_metrics_CamCAN.csv.gz\")\n", "print(\"Raw shape:\", df_raw.shape)\n", "display(df_raw.head())\n"]}, {"cell_type": "code", "execution_count": null, "id": "618e283c", "metadata": {}, "outputs": [], "source": ["# Nettoyage minimal\n", "df_raw = df_raw[~df_raw['bundle'].isin(['left_ventricle', 'right_ventricle'])].copy()\n", "print(\"Sans ventricules :\", df_raw.shape)\n"]}, {"cell_type": "code", "execution_count": null, "id": "6ab6487f", "metadata": {}, "outputs": [], "source": ["def compute_zscore(df, value_col=\"mean_no_cov\"):\n", "    stats = (df.groupby(\"metric_bundle\")[value_col]\n", "               .agg(['mean', 'std'])\n", "               .rename(columns={'mean': 'global_mean', 'std': 'global_std'}))\n", "    stats['global_std'] = stats['global_std'].replace(0, 1e-6)\n", "    df = df.merge(stats, on=\"metric_bundle\", how=\"left\")\n", "    df[\"zscore\"] = (df[value_col] - df[\"global_mean\"]) / df[\"global_std\"]\n", "    return df.drop(columns=[\"global_mean\", \"global_std\"])\n", "\n", "\n", "def gen_sites_for_mlp(df):\n", "    sample_sizes = [5,10,20,30,100,150]  # Différentes tailles d'échantillon\n", "    sample_sizes = [30,100,150]  # Différentes tailles d'échantillon\n", "    sample_sizes = [100]  # Différentes tailles d'échantillon\n", "    disease_ratios = [0.03, 0.1, 0.3, 0.5, 0.7, 0.8]  # Différents pourcentages de malades\n", "    num_tests = 20  # Nombre de tests à effectuer pour chaque combinaison\n", "    n_jobs_number=-1\n", "  \n", "    dfs = generate_sites_no_file(sample_sizes, disease_ratios, num_tests, df,  disease=None, n_jobs=n_jobs_number)\n", "    ret = pd.DataFrame()\n", "    for i, df in enumerate(dfs):\n", "        df[\"sid\"] = df[\"sid\"].astype(str) + str(i)\n", "        d = remove_covariates_effects_metrics(df)\n", "        d = compute_zscore(d)\n", "        ret = pd.concat([ret, d], ignore_index=True)\n", "    return ret\n", "        \n", "        \n", "        "]}, {"cell_type": "code", "execution_count": null, "id": "eec9379b", "metadata": {}, "outputs": [], "source": ["# Augment Data\n", "\n", "df_train, df_temp = split_train_test(df_raw, test_size=0.2, random_state=None)\n", "\n", "df_val, df_test = split_train_test(df_temp, test_size=0.5, random_state=None)\n", "\n", "df_train = augment_df(df_train, 5)\n", "df_train = gen_sites_for_mlp(df_train)\n", "\n", "df_val = augment_df(df_val, 8)\n", "df_val = gen_sites_for_mlp(df_val)\n", "df_test = augment_df(df_test, 8)\n", "df_test = gen_sites_for_mlp(df_test)"]}, {"cell_type": "code", "execution_count": null, "id": "879f76c8", "metadata": {}, "outputs": [], "source": ["# ----- 3. Feature engineering -----\n", "\n", "def build_feature_matrix(df, value_col=\"zscore\", bundle_col=\"metric_bundle\", healthy_tag=\"HC\"):\n", "    features = df.pivot(index=\"sid\", columns=bundle_col, values=value_col)\n", "    label = (df.groupby(\"sid\")[\"disease\"].first().ne(healthy_tag).astype(int))\n", "    mat = features.assign(label=label).reset_index(drop=False)\n", "    return mat\n", "\n", "def make_X_Y(df, value_col=\"zscore\"):\n", "    df = compute_zscore(df, value_col=\"mean_no_cov\")\n", "    df_mat = build_feature_matrix(df, value_col=value_col)\n", "    df_mat = df_mat.drop(columns=[\"sid\"])\n", "    X = df_mat.drop(columns=\"label\").values.astype(np.float32)\n", "    y = df_mat[\"label\"].values.astype(np.float32)\n", "    show_class_balance(y)\n", "    return X, y\n", "\n", "df_train \n", "dupes = (df_train\n", "         .groupby([\"sid\", \"metric_bundle\"])\n", "         .size()\n", "         .loc[lambda s: s > 1]\n", "         .sort_values(ascending=False))\n", "print(f\"Nombre de paires sid / metric_bundle en double : {dupes.shape[0]}\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "d670d008", "metadata": {}, "outputs": [], "source": ["# ----- 4. Split & normalisation -----\n", "X_train, y_train = make_X_Y(df_train)\n", "X_val, y_val = make_X_Y(df_val)\n", "X_test, y_test = make_X_Y(df_test)\n", "\n", "# X_train, X_temp, y_train, y_temp = train_test_split(\n", "#     X, y, test_size=0.5, stratify=y, random_state=SEED)\n", "# X_val, X_test, y_val, y_test = train_test_split(\n", "#     X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED)\n", "\n", "# scaler = StandardScaler().fit(X_train)\n", "# X_train = scaler.transform(X_train)\n", "# X_val   = scaler.transform(X_val)\n", "# X_test  = scaler.transform(X_test)\n", "\n", "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"]}, {"cell_type": "code", "execution_count": null, "id": "4fc2e25f", "metadata": {}, "outputs": [], "source": ["# ----- 5. DataLoader -----\n", "class PatientDataset(Dataset):\n", "    def __init__(self, X, y):\n", "        self.X = torch.tensor(X, dtype=torch.float32)\n", "        self.y = torch.tensor(y, dtype=torch.float32)\n", "    def __len__(self): return len(self.X)\n", "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n", "\n", "BATCH = TRAIN_CFG.get(\"batch_size\", 64)\n", "train_dl = DataLoader(PatientDataset(X_train, y_train), batch_size=BATCH, shuffle=True)\n", "val_dl   = DataLoader(PatientDataset(X_val,   y_val),   batch_size=BATCH)\n", "test_dl  = DataLoader(PatientDataset(X_test,  y_test),  batch_size=BATCH)\n"]}, {"cell_type": "code", "execution_count": null, "id": "d75390b8", "metadata": {}, "outputs": [], "source": ["# ----- 6A. Baseline LogisticRegression -----\n", "from sklearn.linear_model import LogisticRegression\n", "baseline = LogisticRegression(max_iter=1000, n_jobs=-1)\n", "baseline.fit(X_train, y_train)\n", "prob_val = baseline.predict_proba(X_val)[:,1]\n", "auc_base = roc_auc_score(y_val, prob_val)\n", "print(f\"AUC validation LogisticRegression: {auc_base:.3f}\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "da0ef683", "metadata": {}, "outputs": [], "source": ["# ----- 7. Training helpers (importés) -----\n", "from robust_evaluation_tools.MLP_train import (train_epoch, eval_epoch, fit)\n", "NEG_WEIGHT = TRAIN_CFG.get(\"neg_weight\", 10.0)\n"]}, {"cell_type": "code", "execution_count": null, "id": "9ee92186", "metadata": {}, "outputs": [], "source": ["# ----- 8. Hyperparameter tuning (Optuna) -----\n", "def objective(trial):\n", "    hidden_dim1 = trial.suggest_int(\"h1\", 128, 512, step=64)\n", "    hidden_dim2 = trial.suggest_int(\"h2\", 64, 256, step=32)\n", "    hidden_dim3 = trial.suggest_int(\"h3\", 32, 128, step=16)\n", "    drop        = 0.5\n", "    lr          = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n", "    wd          = 1e-3\n", "\n", "    model = PatientMLP(hidden_dims=(hidden_dim1, hidden_dim2, hidden_dim3), drop=drop).to(device)\n", "    state, _, _, best_auc = fit(model, train_dl, val_dl,\n", "                                epochs=15, lr=lr, wd=wd,\n", "                                patience=5, run_name=\"tune\")\n", "    return best_auc\n", "\n", "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n", "study.optimize(objective, n_trials=30, show_progress_bar=True)\n", "\n", "print(\"Best AUC:\", study.best_value)\n", "print(\"Best params:\", study.best_params)\n"]}, {"cell_type": "code", "execution_count": null, "id": "9ff67b0a", "metadata": {}, "outputs": [], "source": ["# ----- 9. Entraînement final avec configuration centralisée -----\n", "if USE_OPTUNA:\n", "    best = study.best_params\n", "    model_final = build_mlp_from_config({**MODEL_CFG,\n", "        \"hidden_dims\": [best[\"h1\"], best[\"h2\"], best[\"h3\"]],\n", "    }).to(device)\n", "    lr = float(best[\"lr\"])\n", "    wd = float(TRAIN_CFG.get(\"weight_decay\", 1e-4))\n", "else:\n", "    model_final = build_mlp_from_config(MODEL_CFG).to(device)\n", "    lr = float(TRAIN_CFG[\"lr\"])\n", "    wd = float(TRAIN_CFG[\"weight_decay\"])\n", "\n", "state, train_losses, val_losses, best_auc = fit(\n", "    model_final, train_dl, val_dl,\n", "    epochs=int(TRAIN_CFG[\"epochs\"]), lr=lr, wd=wd,\n", "    patience=int(TRAIN_CFG[\"patience\"]), run_name=RUN_NAME, device=device, neg_weight=float(TRAIN_CFG.get(\"neg_weight\", 10.0))\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "id": "e495388f", "metadata": {}, "outputs": [], "source": ["# Courbes d’apprentissage\n", "plot_curves(train_losses, val_losses, ylabel=\"BCE Loss\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "0a050561", "metadata": {}, "outputs": [], "source": ["# ----- 11. Évaluation finale sur test -----\n", "_, test_auc, test_f1 = eval_epoch(model_final, test_dl, nn.BCEWithLogitsLoss())\n", "print(f\"AUC test: {test_auc:.3f} | F1 test: {test_f1:.3f}\")\n", "\n", "# Confusion matrix\n", "model_final.eval()\n", "preds, labels = [], []\n", "with torch.no_grad():\n", "    for xb, yb in test_dl:\n", "        preds.append(torch.sigmoid(model_final(xb.to(device))).cpu())\n", "        labels.append(yb)\n", "preds = torch.cat(preds).numpy()\n", "labels= torch.cat(labels).numpy()\n", "ConfusionMatrixDisplay.from_predictions(labels, preds>0.5)\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "1e8740c0", "metadata": {}, "outputs": [], "source": ["# ----- 12. Sauvegarde -----\n", "torch.save(state, MODEL_DIR / f\"{RUN_NAME}_weights.pt\")\n", "\n", "if USE_OPTUNA:\n", "    params_to_save = {**MODEL_CFG,\n", "        \"hidden_dims\": [best[\"h1\"], best[\"h2\"], best[\"h3\"]],\n", "        \"lr\": lr,\n", "        \"weight_decay\": wd,\n", "    }\n", "else:\n", "    params_to_save = {**MODEL_CFG,\n", "        \"lr\": lr,\n", "        \"weight_decay\": wd,\n", "    }\n", "\n", "with open(MODEL_DIR / f\"{RUN_NAME}_params.json\", \"w\") as fp:\n", "    json.dump(params_to_save, fp, indent=2)\n", "print(\"Artifacts saved in\", MODEL_DIR)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ----- 13. Exemple d’inférence -----\n", "sample = np.random.rand(430).reshape(1, -1)\n", "with torch.no_grad():\n", "    prob = torch.sigmoid(model_final(torch.tensor(sample, dtype=torch.float32).to(device))).item()\n", "print(f\"Probabilité malade: {prob:.3f}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ----- 14. Explainability (facultatif) -----\n", "# import shap\n", "# explainer = shap.DeepExplainer(model_final, torch.tensor(X_train[:100]).to(device))\n", "# shap_values = explainer.shap_values(torch.tensor(sample_std).to(device))\n", "# shap.summary_plot(shap_values, features=sample_std)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.15"}}, "nbformat": 4, "nbformat_minor": 5}