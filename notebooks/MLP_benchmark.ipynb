{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark MLP – configurations et seuils\n",
    "\n",
    "Ce notebook compare plusieurs configurations de MLP (profondeur, activation, dropout, batch norm) sur un même split, et évalue différents seuils de décision.\n",
    "\n",
    "Guide:\n",
    "- Ajustez les chemins et options dans la cellule de configuration.\n",
    "- Définissez la liste `CONFIGS` (chaque dict = une config modèle).\n",
    "- `THRESHOLDS` contrôle les seuils testés (par ex. F1 au meilleur seuil sur validation).\n",
    "- Les meilleurs résultats sont affichés et, en option, sauvegardés dans `Pytorch_models/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbeb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Données\n",
    "DATA_CSV = Path(\"DONNES_MLP/train_data_all.csv\")  # ajustez si besoin\n",
    "HEALTHY_TAG = \"HC\"\n",
    "RUN_PREFIX = \"bench_mlp\"\n",
    "SEED = 41\n",
    "\n",
    "# Entraînement\n",
    "TRAIN_CFG = {\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 80,\n",
    "    \"patience\": 10,\n",
    "    \"neg_weight\": 10.0,\n",
    "}\n",
    "\n",
    "# Seuils à évaluer\n",
    "THRESHOLDS = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "# Augmentation\n",
    "AUGMENT = True\n",
    "AUG_TRAIN_N = 5\n",
    "AUG_VAL_N = 8\n",
    "AUG_TEST_N = 8\n",
    "\n",
    "# Sauvegarde des meilleurs modèles\n",
    "SAVE_TOP_K = 1\n",
    "\n",
    "# Configurations MLP à comparer (modifiez/ajoutez librement)\n",
    "CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"relu_256x128x64_do0.5_bn\",\n",
    "        \"in_features\": 430,\n",
    "        \"hidden_dims\": [256, 128, 64],\n",
    "        \"activation\": \"relu\",\n",
    "        \"batch_norm\": True,\n",
    "        \"dropout\": 0.5,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gelu_512x256_do0.4_bn\",\n",
    "        \"in_features\": 430,\n",
    "        \"hidden_dims\": [512, 256],\n",
    "        \"activation\": \"gelu\",\n",
    "        \"batch_norm\": True,\n",
    "        \"dropout\": 0.4,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"leaky_512x256x128x64_do_0.1_0.2_0.2_0.1_bn\",\n",
    "        \"in_features\": 430,\n",
    "        \"hidden_dims\": [512, 256, 128, 64],\n",
    "        \"activation\": \"leaky_relu\",\n",
    "        \"batch_norm\": True,\n",
    "        \"dropout\": [0.1, 0.2, 0.2, 0.1],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"elu_256x64_do0.3_noBN\",\n",
    "        \"in_features\": 430,\n",
    "        \"hidden_dims\": [256, 64],\n",
    "        \"activation\": \"elu\",\n",
    "        \"batch_norm\": False,\n",
    "        \"dropout\": 0.3,\n",
    "    },\n",
    "]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import json, time\n",
    "\n",
    "from robust_evaluation_tools.robust_MLP import build_mlp_from_config, MODEL_DIR\n",
    "from robust_evaluation_tools.synthectic_sites_generations import augment_df\n",
    "from robust_evaluation_tools.MLP_train import (\n",
    "    PatientDataset, make_loaders, train_epoch, eval_epoch, fit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chargement et préparation des données ---\n",
    "assert DATA_CSV.exists(), f\"Fichier introuvable: {DATA_CSV}\"\n",
    "df_raw = pd.read_csv(DATA_CSV)\n",
    "# Retire les ventricules si présents\n",
    "if 'bundle' in df_raw.columns:\n",
    "    df_raw = df_raw[~df_raw['bundle'].isin(['left_ventricle', 'right_ventricle'])].copy()\n",
    "print('Shape après filtre:', df_raw.shape)\n",
    "\n",
    "# Helpers de features\n",
    "def compute_zscore(df, value_col=\"mean_no_cov\"):\n",
    "    stats = (df.groupby(\"metric_bundle\")[value_col]\n",
    "               .agg(['mean', 'std'])\n",
    "               .rename(columns={'mean': 'global_mean', 'std': 'global_std'}))\n",
    "    stats['global_std'] = stats['global_std'].replace(0, 1e-6)\n",
    "    df = df.merge(stats, on=\"metric_bundle\", how=\"left\")\n",
    "    df[\"zscore\"] = (df[value_col] - df[\"global_mean\"]) / df[\"global_std\"]\n",
    "    return df.drop(columns=[\"global_mean\", \"global_std\"])\n",
    "\n",
    "def build_feature_matrix(df, value_col=\"zscore\", bundle_col=\"metric_bundle\", healthy_tag=HEALTHY_TAG):\n",
    "    features = df.pivot(index=\"sid\", columns=bundle_col, values=value_col)\n",
    "    label = (df.groupby(\"sid\")[\"disease\"].first().ne(healthy_tag).astype(int))\n",
    "    mat = features.assign(label=label).reset_index(drop=False)\n",
    "    return mat\n",
    "\n",
    "def make_X_Y(df, value_col=\"zscore\"):\n",
    "    df = compute_zscore(df, value_col=\"mean_no_cov\")\n",
    "    df_mat = build_feature_matrix(df, value_col=value_col)\n",
    "    df_mat = df_mat.drop(columns=[\"sid\"])\n",
    "    X = df_mat.drop(columns=\"label\").values.astype(np.float32)\n",
    "    y = df_mat[\"label\"].values.astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "# Split train/val/test (50/25/25) stratifie si possible\n",
    "from sklearn.model_selection import train_test_split\n",
    "sid_label = (df_raw.groupby('sid')['disease'].first().ne(HEALTHY_TAG).astype(int))\n",
    "sids = sid_label.index.values\n",
    "labels = sid_label.values\n",
    "s_train, s_temp, y_train_sid, y_temp_sid = train_test_split(sids, labels, test_size=0.5, stratify=labels, random_state=SEED)\n",
    "s_val, s_test, y_val_sid, y_test_sid = train_test_split(s_temp, y_temp_sid, test_size=0.5, stratify=y_temp_sid, random_state=SEED)\n",
    "\n",
    "df_train = df_raw[df_raw['sid'].isin(s_train)].copy()\n",
    "df_val   = df_raw[df_raw['sid'].isin(s_val)].copy()\n",
    "df_test  = df_raw[df_raw['sid'].isin(s_test)].copy()\n",
    "\n",
    "# Augmentation optionnelle\n",
    "if AUGMENT:\n",
    "    df_train = augment_df(df_train, int(AUG_TRAIN_N))\n",
    "    df_val   = augment_df(df_val,   int(AUG_VAL_N))\n",
    "    df_test  = augment_df(df_test,  int(AUG_TEST_N))\n",
    "\n",
    "X_train, y_train = make_X_Y(df_train)\n",
    "X_val,   y_val   = make_X_Y(df_val)\n",
    "X_test,  y_test  = make_X_Y(df_test)\n",
    "\n",
    "print('Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2389bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DataLoader (importés) ---\n",
    "from robust_evaluation_tools.MLP_train import make_loaders\n",
    "train_dl, val_dl, test_dl = make_loaders(X_train, y_train, X_val, y_val, X_test, y_test, int(TRAIN_CFG[\"batch_size\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f503589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Entraînement (importés) ---\n",
    "from robust_evaluation_tools.MLP_train import train_epoch, eval_epoch, fit\n",
    "NEG_WEIGHT = float(TRAIN_CFG.get(\"neg_weight\", 10.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501cc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Benchmark des configurations ---\n",
    "results = []\n",
    "crit = nn.BCEWithLogitsLoss(reduction='none')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for cfg in CONFIGS:\n",
    "    cfg_name = cfg.get('name', 'cfg')\n",
    "    run_name = f\"{RUN_PREFIX}_{cfg_name}\"\n",
    "    model = build_mlp_from_config(cfg).to(device)\n",
    "    model, tr_losses, val_losses, best_auc = fit(\n",
    "        model, train_dl, val_dl,\n",
    "        epochs=int(TRAIN_CFG['epochs']),\n",
    "        lr=float(TRAIN_CFG['lr']),\n",
    "        wd=float(TRAIN_CFG['weight_decay']),\n",
    "        patience=int(TRAIN_CFG['patience']),\n",
    "    )\n",
    "    # Validation metrics + threshold sweep\n",
    "    val_loss, val_auc, _, val_probs, val_labels = eval_epoch(model, val_dl, crit)\n",
    "    best_f1, best_thr = -1.0, None\n",
    "    for thr in THRESHOLDS:\n",
    "        f1 = f1_score(val_labels, (val_probs > thr).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    # Test metrics at best validation threshold\n",
    "    test_loss, test_auc, _, test_probs, test_labels = eval_epoch(model, test_dl, crit)\n",
    "    test_f1 = f1_score(test_labels, (test_probs > best_thr).astype(int)) if best_thr is not None else float('nan')\n",
    "\n",
    "    results.append({\n",
    "        'name': cfg_name,\n",
    "        'val_auc': float(val_auc),\n",
    "        'val_best_f1': float(best_f1),\n",
    "        'val_best_thr': float(best_thr if best_thr is not None else 0.5),\n",
    "        'test_auc': float(test_auc),\n",
    "        'test_f1_at_best_thr': float(test_f1),\n",
    "        'config': cfg,\n",
    "        'run_name': run_name,\n",
    "        'state_dict': model.state_dict(),\n",
    "    })\n",
    "\n",
    "# Tri des résultats par val_auc puis val_best_f1\n",
    "results_sorted = sorted(results, key=lambda r: (r['val_auc'], r['val_best_f1']), reverse=True)\n",
    "pd.DataFrame([{k: v for k, v in r.items() if k not in ('config','state_dict')} for r in results_sorted])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sauvegarde des meilleurs modèles (optionnel) ---\n",
    "to_save = results_sorted[:int(SAVE_TOP_K)] if SAVE_TOP_K and len(results_sorted) else []\n",
    "saved = []\n",
    "for r in to_save:\n",
    "    run_name = r['run_name']\n",
    "    state = r['state_dict']\n",
    "    torch.save(state, MODEL_DIR / f\"{run_name}_weights.pt\")\n",
    "    params_to_save = {**r['config'], 'lr': float(TRAIN_CFG['lr']), 'weight_decay': float(TRAIN_CFG['weight_decay'])}\n",
    "    with open(MODEL_DIR / f\"{run_name}_params.json\", 'w') as fp:\n",
    "        json.dump(params_to_save, fp, indent=2)\n",
    "    saved.append(run_name)\n",
    "\n",
    "print('Saved runs:', saved)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
