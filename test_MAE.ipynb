{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS and UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/local/USHERBROOKE/davy3001/Documents/COMBAT/Jodoin/Combat_robust']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/COMBAT/Jodoin/Combat_robust/.robust/lib/python3.10/site-packages/IPython/core/async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/Documents/COMBAT/Jodoin/Combat_robust/.robust/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3301\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3298\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile \u001b[38;5;28;01mif\u001b[39;00m shell_futures \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_class()\n\u001b[1;32m   3300\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 3301\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_trap:\n\u001b[1;32m   3304\u001b[0m         \u001b[38;5;66;03m# Compile to bytecode\u001b[39;00m\n\u001b[1;32m   3305\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/COMBAT/Jodoin/Combat_robust/.robust/lib/python3.10/site-packages/IPython/core/compilerop.py:155\u001b[0m, in \u001b[0;36mCachingCompiler.cache\u001b[0;34m(self, transformed_code, number, raw_code)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     raw_code \u001b[38;5;241m=\u001b[39m transformed_code\n\u001b[0;32m--> 155\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_code_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Save the execution count\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_map[name] \u001b[38;5;241m=\u001b[39m number\n",
      "File \u001b[0;32m~/Documents/COMBAT/Jodoin/Combat_robust/.robust/lib/python3.10/site-packages/ipykernel/compiler.py:105\u001b[0m, in \u001b[0;36mXCachingCompiler.get_code_name\u001b[0;34m(self, raw_code, code, number)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_code_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_code, code, number):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the code name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_file_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/COMBAT/Jodoin/Combat_robust/.robust/lib/python3.10/site-packages/ipykernel/compiler.py:91\u001b[0m, in \u001b[0;36mget_file_name\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cell_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     name \u001b[38;5;241m=\u001b[39m murmur2_x86(code, get_tmp_hash_seed())\n\u001b[0;32m---> 91\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_tmp_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell_name\n",
      "File \u001b[0;32m~/Documents/COMBAT/Jodoin/Combat_robust/.robust/lib/python3.10/site-packages/ipykernel/compiler.py:76\u001b[0m, in \u001b[0;36mget_tmp_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tmp_directory\u001b[39m():\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a temp directory.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     tmp_dir \u001b[38;5;241m=\u001b[39m convert_to_long_pathname(\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     pid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp_dir \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipykernel_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(pid)\n",
      "File \u001b[0;32m~/Documents/COMBAT/Jodoin/Combat_robust/.robust/lib/python3.10/tempfile.py:315\u001b[0m, in \u001b[0;36mgettempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgettempdir\u001b[39m():\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns tempfile.tempdir as str.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mfsdecode(\u001b[43m_gettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/COMBAT/Jodoin/Combat_robust/.robust/lib/python3.10/tempfile.py:308\u001b[0m, in \u001b[0;36m_gettempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tempdir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m         tempdir \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_tempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     _once_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Documents/COMBAT/Jodoin/Combat_robust/.robust/lib/python3.10/tempfile.py:223\u001b[0m, in \u001b[0;36m_get_default_tempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# no point trying more names in this directory\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(_errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    224\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo usable temporary directory found in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    225\u001b[0m                         dirlist)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/local/USHERBROOKE/davy3001/Documents/COMBAT/Jodoin/Combat_robust']"
     ]
    }
   ],
   "source": [
    "print(\"hi\")\n",
    "\n",
    "import os, math\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from scripts import combat_info\n",
    "from scripts import combat_quick_apply\n",
    "from scripts import combat_quick_QC\n",
    "from robust_evaluation_tools.robust_utils import get_site, robust_text, rwp_text, get_camcan_file, get_diseases, get_metrics, add_nb_patients_and_diseased\n",
    "from robust_evaluation_tools.robust_harmonization import fit, apply, visualize_harmonization, QC, compare_with_compilation, create_presentation, compare_distances, compare_with_compilation_var\n",
    "from robust_evaluation_tools.synthectic_sites_generations import generate_sites\n",
    "from robust_evaluation_tools.robust_outlier_detection import z_score_detection, flag_sid\n",
    "\n",
    "MAINFOLDER = \"RESULTS/MAE_TEST\"\n",
    "SYNTHETIC_SITES = f\"{MAINFOLDER}/SYNTHETIC_SITES\"\n",
    "\n",
    "ANALYSIS_FOLDER = f\"{MAINFOLDER}/ANALYSIS\"\n",
    "\n",
    "robust_methods_for_analysis = [\"No\",\"raw\", \"IQR\",'MAD','MMS', 'VS', 'VS2', 'FLIP', 'Z_SCORE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HARMONIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize(f_train, ref_data_file, metric,harmonizartion_method, f_test, directory, method, robust, rwp,hc):\n",
    "     \n",
    "    if method == 'robust':\n",
    "        dir =os.path.join(directory,robust)\n",
    "    else:\n",
    "        dir = os.path.join(directory,method)\n",
    "    print(f_train)\n",
    "    \n",
    "    if robust == 'raw':\n",
    "        output_filename_train = f_train\n",
    "        output_filename_test = f_test\n",
    "    else:\n",
    "        # Fit the model\n",
    "        output_model_filename = fit(f_train, ref_data_file, metric, harmonizartion_method, robust, rwp, dir, hc,)\n",
    "        # Apply the model\n",
    "        output_filename_train = apply(f_train, output_model_filename, metric, harmonizartion_method, robust, rwp, dir)\n",
    "        output_filename_test = apply(f_test, output_model_filename, metric, harmonizartion_method, robust, rwp, dir)\n",
    "    \n",
    "    # Visualize the harmonization\n",
    "    #visualize_harmonization(f_test, output_filename, ref_data_file, dir, bundles = '')\n",
    "    mae_test = compare_with_compilation(pd.read_csv(output_filename_test))\n",
    "    maev_test = compare_with_compilation_var(pd.read_csv(output_filename_test))\n",
    "\n",
    "    mae_train = compare_with_compilation(pd.read_csv(output_filename_train))\n",
    "    maev_train = compare_with_compilation_var(pd.read_csv(output_filename_train))\n",
    "\n",
    "    mae_test['site'] = get_site(f_train)\n",
    "    mae_test['method'] = method\n",
    "    mae_test['robust_method'] = robust\n",
    "\n",
    "    maev_test['site'] = get_site(f_train)\n",
    "    maev_test['method'] = method\n",
    "    maev_test['robust_method'] = robust\n",
    "\n",
    "    mae_train['site'] = get_site(f_train)\n",
    "    mae_train['method'] = method\n",
    "    mae_train['robust_method'] = robust\n",
    "\n",
    "    maev_train['site'] = get_site(f_train)\n",
    "    maev_train['method'] = method\n",
    "    maev_train['robust_method'] = robust\n",
    "    \n",
    "    return mae_test, maev_test, mae_train, maev_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_site(f_train,f_test, robust_methods, directory, ref_data_file, metric,harmonizartion_method):\n",
    "    # 4 harmonization\n",
    "    mae_test_raw, maev_test_raw, mae_train_raw, maev_train_raw = harmonize(f_train, ref_data_file, metric, harmonizartion_method, f_test, directory, \"raw\", \"raw\", False, False)\n",
    "    mae_test_hc, maev_test_hc, mae_train_hc, maev_train_hc = harmonize(f_train, ref_data_file, metric, harmonizartion_method, f_test, directory, \"hc\", \"No\", False, True)\n",
    "    mae_test_no_robust, maev_test_no_robust, mae_train_no_robust, maev_train_no_robust = harmonize(f_train, ref_data_file, metric, harmonizartion_method, f_test, directory, \"NoRobust\", \"No\", False, False)\n",
    "    df_mae_test_robust = pd.DataFrame()\n",
    "    df_maev_test_robust = pd.DataFrame()\n",
    "    df_mae_train_robust = pd.DataFrame()\n",
    "    df_maev_train_robust = pd.DataFrame()\n",
    "    \n",
    "    for robust in robust_methods:\n",
    "        mae_test_robust, maev_test_robust, mae_train_robust, maev_train_robust = harmonize(f_train, ref_data_file, metric, harmonizartion_method, f_test, directory, \"robust\", robust, False, False)\n",
    "        mae_test_robust_rwp, maev_test_robust_rwp, mae_train_robust_rwp, maev_train_robust_rwp = harmonize(f_train, ref_data_file, metric, harmonizartion_method, f_test, directory, \"robust_rwp\", robust, True, False)\n",
    "\n",
    "        df_mae_test_robust = pd.concat(\n",
    "            [df_mae_test_robust, mae_test_robust, mae_test_robust_rwp],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        df_maev_test_robust = pd.concat(\n",
    "            [df_maev_test_robust, maev_test_robust, maev_test_robust_rwp],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        df_mae_train_robust = pd.concat(\n",
    "            [df_mae_train_robust, mae_train_robust, mae_train_robust_rwp],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        df_maev_train_robust = pd.concat(\n",
    "            [df_maev_train_robust, maev_train_robust, maev_train_robust_rwp],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Combine MAE Test in a single DataFrame\n",
    "    mae_combined_test = pd.concat([mae_test_raw, mae_test_hc, mae_test_no_robust, df_mae_train_robust], ignore_index=True)\n",
    "\n",
    "    # Combine MAEV Test in a single DataFrame\n",
    "    maev_combined_test = pd.concat([maev_test_raw, maev_test_hc, maev_test_no_robust, df_maev_test_robust], ignore_index=True)\n",
    "\n",
    "    # Combine MAE Train in a single DataFrame\n",
    "    mae_combined_train = pd.concat([mae_train_raw, mae_train_hc, mae_train_no_robust, df_mae_train_robust], ignore_index=True)\n",
    "\n",
    "    # Combine MAEV Train in a single DataFrame\n",
    "    maev_combined_train = pd.concat([maev_train_raw, maev_train_hc, maev_train_no_robust, df_maev_train_robust], ignore_index=True)\n",
    "\n",
    "\n",
    "    return mae_combined_test, maev_combined_test, mae_combined_train, maev_combined_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to analyze a single (sample_size, disease_ratio) combination\n",
    "def process_analysis(disease, sample_size, disease_ratio, test_index, harmonization_method, SYNTHETIC_SITES_VERSION, metrics, robust_methods):\n",
    "    \n",
    "    directory = os.path.join(MAINFOLDER,'PROCESS', disease)\n",
    "    directory_site = os.path.join(SYNTHETIC_SITES, SYNTHETIC_SITES_VERSION,disease)\n",
    "    \n",
    "    sizeDir = os.path.join(directory, f\"{sample_size}_{int(disease_ratio * 100)}\", f\"{test_index}\")\n",
    "    sizeDir_site = os.path.join(directory_site, f\"{sample_size}_{int(disease_ratio * 100)}\")\n",
    "    tempDir_site = os.path.join(sizeDir_site, f\"{test_index}\")\n",
    "\n",
    "    mae_compilation_test = pd.DataFrame()\n",
    "    maev_compilation_test = pd.DataFrame()\n",
    "    mae_compilation_train = pd.DataFrame()\n",
    "    maev_compilation_train = pd.DataFrame()\n",
    "\n",
    "    z_score_sids = z_score_detection(os.path.join(tempDir_site, f\"train_{sample_size}_{int(disease_ratio * 100)}_{test_index}_all.csv\"))\n",
    "\n",
    "    for metric in metrics:\n",
    "        tempDir = os.path.join(sizeDir, metric)\n",
    "        os.makedirs(tempDir, exist_ok=True)\n",
    "\n",
    "        train_file_name = f\"train_{sample_size}_{int(disease_ratio * 100)}_{test_index}_{metric}.csv\"\n",
    "        test_file_name = f\"test_{sample_size}_{int(disease_ratio * 100)}_{test_index}_{metric}.csv\"\n",
    "\n",
    "        # Load and save training dataset\n",
    "        train_file = os.path.join(tempDir_site, train_file_name)\n",
    "        train_df = pd.read_csv(train_file)\n",
    "        train_df = train_df[~train_df['bundle'].isin(['left_ventricle', 'right_ventricle'])]\n",
    "        train_df = train_df.drop(columns=['mean_no_cov', 'metric_bundle'])\n",
    "        train_df = flag_sid(train_df, z_score_sids, \"Z_SCORE\")\n",
    "        new_train_file = os.path.join(tempDir, train_file_name)\n",
    "        train_df.to_csv(new_train_file, index=False)\n",
    "        \n",
    "\n",
    "        # Load and save test dataset\n",
    "        test_file = os.path.join(tempDir_site, test_file_name)\n",
    "        test_df = pd.read_csv(test_file)\n",
    "        test_df = test_df[~test_df['bundle'].isin(['left_ventricle', 'right_ventricle'])]\n",
    "        test_df = test_df.drop(columns=['mean_no_cov', 'metric_bundle'])\n",
    "        new_test_file = os.path.join(tempDir, test_file_name)\n",
    "        test_df.to_csv(new_test_file, index=False)\n",
    "\n",
    "        ref_data_file = get_camcan_file(metric)\n",
    "\n",
    "        # Analyze the site\n",
    "        mae_analyze_test, maev_analyze_test, mae_analyze_train, maev_analyze_train = analyse_site(\n",
    "                    new_train_file, new_test_file, robust_methods, tempDir, ref_data_file, metric, harmonization_method\n",
    "                )\n",
    "\n",
    "        # Add robust_method, disease, and metric information to the analyze DataFrames\n",
    "        mae_analyze_test['disease'] = disease\n",
    "        mae_analyze_test['metric'] = metric\n",
    "\n",
    "        maev_analyze_test['disease'] = disease\n",
    "        maev_analyze_test['metric'] = metric\n",
    "\n",
    "        mae_analyze_train['disease'] = disease\n",
    "        mae_analyze_train['metric'] = metric\n",
    "\n",
    "        maev_analyze_train['disease'] = disease\n",
    "        maev_analyze_train['metric'] = metric\n",
    "\n",
    "        # Create new compilations for test and train datasets\n",
    "        mae_compilation_test = pd.concat([mae_compilation_test, mae_analyze_test], ignore_index=True)\n",
    "        maev_compilation_test = pd.concat([maev_compilation_test, maev_analyze_test], ignore_index=True)\n",
    "\n",
    "        mae_compilation_train = pd.concat([mae_compilation_train, mae_analyze_train], ignore_index=True)\n",
    "        maev_compilation_train = pd.concat([maev_compilation_train, maev_analyze_train], ignore_index=True)\n",
    "\n",
    "    # Save the compilations for test and train datasets\n",
    "    os.makedirs(sizeDir, exist_ok=True)\n",
    "    mae_test_file_path = os.path.join(sizeDir, \"mae_compilation_test.csv\")\n",
    "    maev_test_file_path = os.path.join(sizeDir, \"maev_compilation_test.csv\")\n",
    "    mae_train_file_path = os.path.join(sizeDir, \"mae_compilation_train.csv\")\n",
    "    maev_train_file_path = os.path.join(sizeDir, \"maev_compilation_train.csv\")\n",
    "\n",
    "    mae_compilation_test.to_csv(mae_test_file_path, index=False)\n",
    "    maev_compilation_test.to_csv(maev_test_file_path, index=False)\n",
    "    mae_compilation_train.to_csv(mae_train_file_path, index=False)\n",
    "    maev_compilation_train.to_csv(maev_train_file_path, index=False)\n",
    "\n",
    "    # Return the file paths for test and train datasets\n",
    "    return (mae_test_file_path, maev_test_file_path, mae_train_file_path, maev_train_file_path)\n",
    "\n",
    "# Parallelized analysis method (excluding num_tests from parallelization)\n",
    "def analyse_method(sample_sizes, disease_ratios, num_tests, robust_methods,diseases, metrics, harmonization_method, SYNTHETIC_SITES_VERSION, n_jobs=-1):\n",
    "    # Generate all task combinations (excluding num_tests)\n",
    "    tasks = [\n",
    "        (disease, sample_size, disease_ratio, num_test, harmonization_method, SYNTHETIC_SITES_VERSION, metrics, robust_methods)\n",
    "        for disease in diseases\n",
    "        for sample_size in sample_sizes\n",
    "        for disease_ratio in disease_ratios\n",
    "        for num_test in range(num_tests)\n",
    "    ]\n",
    "\n",
    "    # Run all combinations in parallel and collect file paths\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(process_analysis)(*task) for task in tasks)\n",
    "\n",
    "    # Separate the list of tuples into four lists of file paths\n",
    "    mae_test_file_paths = [res[0] for res in results]\n",
    "    maev_test_file_paths = [res[1] for res in results]\n",
    "    mae_train_file_paths = [res[2] for res in results]\n",
    "    maev_train_file_paths = [res[3] for res in results]\n",
    "\n",
    "    # Concatenate all mae compilations for test datasets\n",
    "    mae_compilation_test_global = pd.concat(\n",
    "        [pd.read_csv(fpath) for fpath in mae_test_file_paths],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Concatenate all maev compilations for test datasets\n",
    "    maev_compilation_test_global = pd.concat(\n",
    "        [pd.read_csv(fpath) for fpath in maev_test_file_paths],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Concatenate all mae compilations for train datasets\n",
    "    mae_compilation_train_global = pd.concat(\n",
    "        [pd.read_csv(fpath) for fpath in mae_train_file_paths],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Concatenate all maev compilations for train datasets\n",
    "    maev_compilation_train_global = pd.concat(\n",
    "        [pd.read_csv(fpath) for fpath in maev_train_file_paths],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Final save of the four compiled DataFrames\n",
    "    directory = os.path.join(MAINFOLDER)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    mae_compilation_test_global.to_csv(os.path.join(directory, \"mae_compilation_test_global.csv\"), index=False)\n",
    "    maev_compilation_test_global.to_csv(os.path.join(directory, \"maev_compilation_test_global.csv\"), index=False)\n",
    "    mae_compilation_train_global.to_csv(os.path.join(directory, \"mae_compilation_train_global.csv\"), index=False)\n",
    "    maev_compilation_train_global.to_csv(os.path.join(directory, \"maev_compilation_train_global.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sites_for_disease(disease, SYNTHETIC_SITES, SYNTHETIC_SITES_VERSION, sample_sizes, disease_ratios, num_tests, n_jobs=-1):\n",
    "    # Load data for the disease\n",
    "    data_path = os.path.join('DONNES','COMPILATIONS_AUG_3', f'{disease}_combination_all_metrics_CamCAN.csv.gz')\n",
    "\n",
    "    # Define site directory\n",
    "    directory_site = os.path.join(SYNTHETIC_SITES, SYNTHETIC_SITES_VERSION, disease)\n",
    "\n",
    "    # Generate synthetic sites\n",
    "    generate_sites(sample_sizes, disease_ratios, num_tests, directory_site, data_path, disease=None, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXECUTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonization_method= \"classic\"\n",
    "\n",
    "SYNTHETIC_SITES_VERSION = \"v1\"\n",
    "\n",
    "metrics = get_metrics()\n",
    "#diseases = get_diseases(True)\n",
    "diseases = [\"SCHZ\", \"AD\", \"TBI\"]\n",
    "robust_methods = [\"IQR\",'MAD','MMS', 'VS','FLIP', 'Z_SCORE']\n",
    "#robust_methods = [\"MMS\",\"IQR\",'MAD', 'VS', 'VS2', 'TOP30', 'FLIP']\n",
    "#'Z_SCORE'\n",
    "\n",
    "\n",
    "sample_sizes = [5,10,20,30,100,150]  # Différentes tailles d'échantillon\n",
    "disease_ratios = [0.03, 0.1, 0.3, 0.5]  # Différents pourcentages de malades\n",
    "num_tests = 10  # Nombre de tests à effectuer pour chaque combinaison\n",
    "n_jobs=-1\n",
    "\n",
    "for disease in diseases:\n",
    "    generate_sites_for_disease(\n",
    "        disease, SYNTHETIC_SITES, SYNTHETIC_SITES_VERSION, sample_sizes, disease_ratios, num_tests, n_jobs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_method(sample_sizes, disease_ratios, num_tests, robust_methods,diseases, metrics, harmonization_method, SYNTHETIC_SITES_VERSION, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mae_or_maev_compilations(mainfolder, diseases, sample_sizes, disease_ratios, num_tests, mae_or_maev='mae'):\n",
    "    tests, trains = [], []\n",
    "    for d in diseases:\n",
    "        for s in sample_sizes:\n",
    "            for r in disease_ratios:\n",
    "                for i in range(num_tests):\n",
    "                    base = os.path.join(mainfolder, \"PROCESS\", d, f\"{s}_{int(r*100)}\", str(i))\n",
    "                    test_path  = os.path.join(base, f\"{mae_or_maev}_compilation_test.csv\")\n",
    "                    train_path = os.path.join(base, f\"{mae_or_maev}_compilation_train.csv\")\n",
    "                    if os.path.isfile(test_path):\n",
    "                        tests.append(pd.read_csv(test_path))\n",
    "                    if os.path.isfile(train_path):\n",
    "                        trains.append(pd.read_csv(train_path))\n",
    "    df_test  = pd.concat(tests,  ignore_index=True) if tests  else pd.DataFrame()\n",
    "    df_train = pd.concat(trains, ignore_index=True) if trains else pd.DataFrame()\n",
    "    return df_test, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files into a dictionary\n",
    "mae_compilations = {}\n",
    "for file_name in [\"mae_compilation_test_global.csv\", \"mae_compilation_train_global.csv\"]:\n",
    "    file_path = os.path.join(MAINFOLDER, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        mae_compilations[file_name] = pd.read_csv(file_path)\n",
    "    else:\n",
    "        mae_or_maev = 'mae' if 'mae' in file_name else 'maev'\n",
    "        df_test, df_train = load_mae_or_maev_compilations(\n",
    "            MAINFOLDER, diseases, sample_sizes, disease_ratios, num_tests, mae_or_maev=mae_or_maev\n",
    "        )\n",
    "        mae_compilations[file_name] = df_test if 'test' in file_name else df_train\n",
    "\n",
    "# Process each file\n",
    "for file_name, mae_compilation in mae_compilations.items():\n",
    "    # Retrieve unique methods\n",
    "    methods = mae_compilation['robust_method'].unique()\n",
    "\n",
    "    # Save a CSV file for each method\n",
    "    for method in methods:\n",
    "        # Filter rows corresponding to the method\n",
    "        method_df = mae_compilation[mae_compilation['robust_method'] == method]\n",
    "        \n",
    "        # Create a folder for the method\n",
    "        method_directory = os.path.join(MAINFOLDER, 'CSV', method)\n",
    "        os.makedirs(method_directory, exist_ok=True)\n",
    "        \n",
    "        # Save the CSV file in the folder\n",
    "        method_file_path = os.path.join(method_directory, f\"{method}_{file_name}\")\n",
    "        method_df.to_csv(method_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files into a dictionary\n",
    "maev_compilations = {}\n",
    "for file_name in [\"maev_compilation_test_global.csv\", \"maev_compilation_train_global.csv\"]:\n",
    "    file_path = os.path.join(MAINFOLDER, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        maev_compilations[file_name] = pd.read_csv(file_path)\n",
    "    else:\n",
    "        mae_or_maev = 'maev' if 'maev' in file_name else 'mae'\n",
    "        df_test, df_train = load_mae_or_maev_compilations(\n",
    "            MAINFOLDER, diseases, sample_sizes, disease_ratios, num_tests, mae_or_maev=mae_or_maev\n",
    "        )\n",
    "        maev_compilations[file_name] = df_test if 'test' in file_name else df_train\n",
    "\n",
    "# Process each file\n",
    "for file_name, maev_compilation in maev_compilations.items():\n",
    "    # Retrieve unique methods\n",
    "    methods = maev_compilation['robust_method'].unique()\n",
    "\n",
    "    # Save a CSV file for each method\n",
    "    for method in methods:\n",
    "        # Filter rows corresponding to the method\n",
    "        method_df = maev_compilation[maev_compilation['robust_method'] == method]\n",
    "        \n",
    "        # Create a folder for the method\n",
    "        method_directory = os.path.join(MAINFOLDER, 'CSV', method)\n",
    "        os.makedirs(method_directory, exist_ok=True)\n",
    "        \n",
    "        # Save the CSV file in the folder\n",
    "        method_file_path = os.path.join(method_directory, f\"{method}_{file_name}\")\n",
    "        method_df.to_csv(method_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_all_method_files(main_folder, methods, context=\"test\"):\n",
    "    all_dfs = []\n",
    "    for m in methods:\n",
    "        file_name = f\"{m}_mae_compilation_{context}_global.csv\"\n",
    "        path = os.path.join(main_folder,'CSV', m, file_name)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            all_dfs.append(df)\n",
    "    return pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_all_method_files_maev(main_folder, methods, context=\"test\"):\n",
    "    all_dfs = []\n",
    "    for m in methods:\n",
    "        file_name = f\"{m}_maev_compilation_{context}_global.csv\"\n",
    "        path = os.path.join(main_folder,'CSV', m, file_name)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            all_dfs.append(df)\n",
    "    return pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mea(df, sample_size, disease, metric, directory, dataset_type):\n",
    "    directory = os.path.join(directory, \"MAE_PLOTS\", disease, metric, str(sample_size), dataset_type)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    df_filtered = df[\n",
    "        (df['num_patients'] == sample_size) & \n",
    "        (df['disease'] == disease) & \n",
    "        (df['metric'] == metric)\n",
    "    ]\n",
    "\n",
    "    # Méthodes de base + méthodes robustes\n",
    "    robust_methods = [m for m in df_filtered['robust_method'].dropna().unique() if m not in [\"No\", \"raw\"]]\n",
    "\n",
    "\n",
    "    #base_methods = [\"raw\", \"hc\", \"no_robust\"]\n",
    "    base_methods = [\"hc\", \"NoRobust\"]\n",
    "    #robust_base_methods = [\"robust\", \"robust_rwp\"]\n",
    "    robust_base_methods = [\"robust\"]\n",
    "    methods = base_methods + [f\"{rb}_{rm}\" for rb in robust_base_methods for rm in robust_methods]\n",
    "    \n",
    "    # Couleurs\n",
    "    method_colors = {\n",
    "        \"raw\": \"grey\",\n",
    "        \"hc\": \"green\",\n",
    "        \"NoRobust\": \"red\"\n",
    "    }\n",
    "    robust_palette = sns.color_palette(\"viridis\", len(robust_methods))\n",
    "    robust_rwp_palette = sns.color_palette(\"magma\", len(robust_methods))\n",
    "    for i, rm in enumerate(robust_methods):\n",
    "        method_colors[f\"robust_{rm}\"] = robust_palette[i]\n",
    "        method_colors[f\"robust_rwp_{rm}\"] = robust_rwp_palette[i]\n",
    "\n",
    "    # Boucle sur les \"bundles\"\n",
    "    for bundle_column in df_filtered.columns:\n",
    "        # On ignore les colonnes non-numériques\n",
    "        if bundle_column in ['site', 'method', 'num_patients', 'disease_ratio',\n",
    "                             'num_diseased', 'metric', 'disease', 'robust_method']:\n",
    "            continue\n",
    "\n",
    "        bundle_df = df_filtered[[bundle_column, 'site', 'method', 'num_patients',\n",
    "                                 'disease_ratio', 'num_diseased', 'robust_method']].copy()\n",
    "        unique_ratios = sorted(bundle_df['disease_ratio'].unique())  # Trié pour être sûr de l'ordre\n",
    "\n",
    "        # *** ICI on paramètre la figure + le positionnement ***\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))  # Ajuster au besoin\n",
    "\n",
    "        # Abscisses pour chaque ratio\n",
    "        x = np.arange(len(unique_ratios))\n",
    "        \n",
    "        # Largeur totale allouée pour le « groupe » de méthodes à chaque ratio\n",
    "        group_width = 0.8  \n",
    "        # On répartit cette largeur entre toutes les méthodes\n",
    "        n_methods = len(methods)\n",
    "        box_width = group_width / n_methods\n",
    "\n",
    "        # Tracé des boxplots pour chaque méthode\n",
    "        for i_m, method in enumerate(methods):\n",
    "            if \"_\" in method and method != \"no_robust\":\n",
    "                method_base, robust_type = method.rsplit(\"_\", 1)\n",
    "                method_df = bundle_df[\n",
    "                    (bundle_df['method'] == method_base) & \n",
    "                    (bundle_df['robust_method'] == robust_type)\n",
    "                ]\n",
    "            else:\n",
    "                method_df = bundle_df[bundle_df['method'] == method]\n",
    "\n",
    "            # On prépare la liste de valeurs par ratio\n",
    "            data = [\n",
    "                method_df[method_df['disease_ratio'] == ratio][bundle_column].values \n",
    "                for ratio in unique_ratios\n",
    "            ]\n",
    "\n",
    "            # Positions: on centre autour de chaque x\n",
    "            # Exemple: x - group_width/2 + (i_m+0.5)*box_width\n",
    "            positions = x - group_width/2 + (i_m + 0.5)*box_width\n",
    "\n",
    "            color = method_colors.get(method, \"black\")\n",
    "            \n",
    "            # S’il y a au moins un point de données\n",
    "            if any(len(d) > 0 for d in data):\n",
    "                ax.boxplot(\n",
    "                    data,\n",
    "                    positions=positions,\n",
    "                    widths=box_width * 0.8,  # Légèrement plus petit que box_width\n",
    "                    patch_artist=True,\n",
    "                    boxprops=dict(facecolor=color, color=color),\n",
    "                    medianprops=dict(color='black')\n",
    "                )\n",
    "\n",
    "        ax.set_xlabel('Prct de patients malades')\n",
    "        ax.set_ylabel('MAE')\n",
    "        ax.set_title(\n",
    "            f\"MAE de l'harmonization selon le pourcentage de patients malades\\n\"\n",
    "            f\"Maladie: {disease}  |  Metric: {metric}  |  Bundle: {bundle_column}\\n\"\n",
    "            f\"Nb patient total: {sample_size} Context: {dataset_type}\"\n",
    "        )\n",
    "\n",
    "        # On place les ticks au milieu de chaque groupe (i.e. sur x)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(unique_ratios)\n",
    "\n",
    "        # Légende manuelle\n",
    "        legend_handles = [\n",
    "            plt.Line2D([0], [0], color=method_colors[m], lw=3, label=f'Method: {m}')\n",
    "            for m in methods\n",
    "        ]\n",
    "        ax.legend(handles=legend_handles, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(directory, f'{bundle_column}_boxplot.png'), bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Load datasets for train and test\n",
    "datasets = {\n",
    "    \"train\": gather_all_method_files(MAINFOLDER, robust_methods_for_analysis, \"train\"),\n",
    "    \"test\": gather_all_method_files(MAINFOLDER, robust_methods_for_analysis, \"test\")\n",
    "}\n",
    "\n",
    "# Process each dataset (train and test) separately\n",
    "for dataset_type, mae_df in datasets.items():\n",
    "    add_nb_patients_and_diseased(mae_df)\n",
    "\n",
    "    # Generate all task combinations\n",
    "    tasks = [\n",
    "        (mae_df, sample_size, disease, metric,ANALYSIS_FOLDER, dataset_type)\n",
    "        for disease in diseases\n",
    "        for sample_size in sample_sizes\n",
    "        for metric in metrics\n",
    "    ]\n",
    "\n",
    "    # # Run all tasks in parallel\n",
    "    # Parallel(n_jobs=-1)(\n",
    "    #     delayed(plot_mea)(*task) for task in tasks\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_meav(df, sample_size, disease, metric, directory, dataset_type):\n",
    "    directory = os.path.join(directory, \"MAEV_PLOTS\", disease, metric, str(sample_size), dataset_type)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    df_filtered = df[\n",
    "        (df['num_patients'] == sample_size) & \n",
    "        (df['disease'] == disease) & \n",
    "        (df['metric'] == metric)\n",
    "    ]\n",
    "\n",
    "    # Méthodes de base + méthodes robustes\n",
    "    robust_methods = [m for m in df_filtered['robust_method'].dropna().unique() if m not in [\"No\", \"raw\"]]\n",
    "\n",
    "    #base_methods = [\"raw\", \"hc\", \"no_robust\"]\n",
    "    base_methods = [\"hc\", \"NoRobust\"]\n",
    "    #robust_base_methods = [\"robust\", \"robust_rwp\"]\n",
    "    robust_base_methods = [\"robust\"]\n",
    "    methods = base_methods + [f\"{rb}_{rm}\" for rb in robust_base_methods for rm in robust_methods]\n",
    "    \n",
    "    # Couleurs\n",
    "    method_colors = {\n",
    "        \"raw\": \"grey\",\n",
    "        \"hc\": \"green\",\n",
    "        \"NoRobust\": \"red\"\n",
    "    }\n",
    "    robust_palette = sns.color_palette(\"viridis\", len(robust_methods))\n",
    "    robust_rwp_palette = sns.color_palette(\"magma\", len(robust_methods))\n",
    "    for i, rm in enumerate(robust_methods):\n",
    "        method_colors[f\"robust_{rm}\"] = robust_palette[i]\n",
    "        method_colors[f\"robust_rwp_{rm}\"] = robust_rwp_palette[i]\n",
    "\n",
    "    # Boucle sur les \"bundles\"\n",
    "    for bundle_column in df_filtered.columns:\n",
    "        # On ignore les colonnes non-numériques\n",
    "        if bundle_column in ['site', 'method', 'num_patients', 'disease_ratio',\n",
    "                             'num_diseased', 'metric', 'disease', 'robust_method']:\n",
    "            continue\n",
    "\n",
    "        bundle_df = df_filtered[[bundle_column, 'site', 'method', 'num_patients',\n",
    "                                 'disease_ratio', 'num_diseased', 'robust_method']].copy()\n",
    "        unique_ratios = sorted(bundle_df['disease_ratio'].unique())  # Trié pour être sûr de l'ordre\n",
    "\n",
    "        # *** ICI on paramètre la figure + le positionnement ***\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))  # Ajuster au besoin\n",
    "\n",
    "        # Abscisses pour chaque ratio\n",
    "        x = np.arange(len(unique_ratios))\n",
    "        \n",
    "        # Largeur totale allouée pour le « groupe » de méthodes à chaque ratio\n",
    "        group_width = 0.8  \n",
    "        # On répartit cette largeur entre toutes les méthodes\n",
    "        n_methods = len(methods)\n",
    "        box_width = group_width / n_methods\n",
    "\n",
    "        # Tracé des boxplots pour chaque méthode\n",
    "        for i_m, method in enumerate(methods):\n",
    "            if \"_\" in method and method != \"no_robust\":\n",
    "                method_base, robust_type = method.rsplit(\"_\", 1)\n",
    "                method_df = bundle_df[\n",
    "                    (bundle_df['method'] == method_base) & \n",
    "                    (bundle_df['robust_method'] == robust_type)\n",
    "                ]\n",
    "            else:\n",
    "                method_df = bundle_df[bundle_df['method'] == method]\n",
    "\n",
    "            # On prépare la liste de valeurs par ratio\n",
    "            data = [\n",
    "                method_df[method_df['disease_ratio'] == ratio][bundle_column].values \n",
    "                for ratio in unique_ratios\n",
    "            ]\n",
    "\n",
    "            # Positions: on centre autour de chaque x\n",
    "            # Exemple: x - group_width/2 + (i_m+0.5)*box_width\n",
    "            positions = x - group_width/2 + (i_m + 0.5)*box_width\n",
    "\n",
    "            color = method_colors.get(method, \"black\")\n",
    "            \n",
    "            # S’il y a au moins un point de données\n",
    "            if any(len(d) > 0 for d in data):\n",
    "                ax.boxplot(\n",
    "                    data,\n",
    "                    positions=positions,\n",
    "                    widths=box_width * 0.8,  # Légèrement plus petit que box_width\n",
    "                    patch_artist=True,\n",
    "                    boxprops=dict(facecolor=color, color=color),\n",
    "                    medianprops=dict(color='black')\n",
    "                )\n",
    "\n",
    "        ax.set_xlabel('Prct de patients malades')\n",
    "        ax.set_ylabel('MAEV')\n",
    "        ax.set_title(\n",
    "            f\"MAEV de l'harmonization selon le pourcentage de patients malades\\n\"\n",
    "            f\"Maladie: {disease}  |  Metric: {metric}  |  Bundle: {bundle_column}\\n\"\n",
    "            f\"Nb patient total: {sample_size} Context: {dataset_type}\"\n",
    "        )\n",
    "\n",
    "        # On place les ticks au milieu de chaque groupe (i.e. sur x)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(unique_ratios)\n",
    "\n",
    "        # Légende manuelle\n",
    "        legend_handles = [\n",
    "            plt.Line2D([0], [0], color=method_colors[m], lw=3, label=f'Method: {m}')\n",
    "            for m in methods\n",
    "        ]\n",
    "        ax.legend(handles=legend_handles, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(directory, f'{bundle_column}_boxplot.png'), bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Load datasets for train and test\n",
    "datasets = {\n",
    "    \"train\": gather_all_method_files_maev(MAINFOLDER, robust_methods_for_analysis, \"train\"),\n",
    "    \"test\": gather_all_method_files_maev(MAINFOLDER, robust_methods_for_analysis, \"test\")\n",
    "}\n",
    "\n",
    "# Process each dataset (train and test) separately\n",
    "for dataset_type, maev_df in datasets.items():\n",
    "    add_nb_patients_and_diseased(maev_df)\n",
    "\n",
    "    # Generate all task combinations\n",
    "    tasks = [\n",
    "        (maev_df, sample_size, disease, metric, ANALYSIS_FOLDER, dataset_type)\n",
    "        for disease in diseases\n",
    "        for sample_size in sample_sizes\n",
    "        for metric in metrics\n",
    "    ]\n",
    "\n",
    "    # # Run all tasks in parallel\n",
    "    # Parallel(n_jobs=-1)(\n",
    "    #     delayed(plot_meav)(*task) for task in tasks\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(ANALYSIS_FOLDER, 'DIFF'), exist_ok=True)\n",
    "# ──────────────────────────────────────────────\n",
    "# 1. Charger le CSV\n",
    "# ──────────────────────────────────────────────\n",
    "df = gather_all_method_files(MAINFOLDER, robust_methods_for_analysis, \"train\")\n",
    "df = df[df[\"method\"] != \"robust_rwp\"]\n",
    "df = df[df[\"method\"] != \"raw\"]\n",
    "\n",
    "meta_cols   = [\"site\", \"method\", \"robust_method\", \"disease\", \"metric\"]\n",
    "bundle_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 2. Préparer la baseline (NoRobust / No)\n",
    "# ──────────────────────────────────────────────\n",
    "baseline_key  = (df[\"method\"] == \"NoRobust\") & (df[\"robust_method\"] == \"No\")\n",
    "baseline_df   = df[baseline_key].copy()\n",
    "\n",
    "# On renomme les colonnes bundle → bundle_base pour pouvoir merger\n",
    "base_rename = {b: f\"{b}_base\" for b in bundle_cols}\n",
    "baseline_df  = baseline_df[[\"disease\", \"site\", \"metric\"] + bundle_cols].rename(columns=base_rename)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 3. Fusionner pour avoir, sur chaque ligne, les valeurs baseline\n",
    "# ──────────────────────────────────────────────\n",
    "df = df.merge(baseline_df, on=[\"disease\", \"site\", \"metric\"], how=\"left\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 4. Différence bundle-par-bundle  (robust − baseline)\n",
    "# ──────────────────────────────────────────────\n",
    "diff_cols = []\n",
    "for b in bundle_cols:\n",
    "    diff_col = f\"{b}_diff\"\n",
    "    df[diff_col] = df[b] - df[f\"{b}_base\"]\n",
    "    diff_cols.append(diff_col)\n",
    "\n",
    "# Moyenne des différences sur l’ensemble des bundles\n",
    "df[\"diff_mean\"] = df[diff_cols].mean(axis=1)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 5. Étiquettes de méthode pour le tableau final\n",
    "# ──────────────────────────────────────────────\n",
    "def label_row(r):\n",
    "    if (r[\"method\"] == \"hc\") and (r[\"robust_method\"] == \"No\"):\n",
    "        return \"hc\"\n",
    "    return r[\"robust_method\"]          # MMS, raw, etc.\n",
    "\n",
    "df[\"method_label\"] = df.apply(label_row, axis=1)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 6. Agrégation finale  (moyenne des diff_mean)\n",
    "# ──────────────────────────────────────────────\n",
    "group_cols = [\"disease\", \"site\", \"metric\", \"method_label\"]\n",
    "agg = df.groupby(group_cols)[\"diff_mean\"].mean()\n",
    "\n",
    "# Pivot → colonnes = method_label\n",
    "table = agg.unstack(\"method_label\")\n",
    "\n",
    "# Ré-ordonner pour mettre \"hc\" en premier (si présent)\n",
    "cols = [\"hc\"] + [c for c in table.columns if c != \"hc\"]\n",
    "table = table[cols]\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 7. Affichage ou export\n",
    "# ──────────────────────────────────────────────\n",
    "for disease in table.index.get_level_values(\"disease\").unique():\n",
    "    subt = table.xs(disease, level=\"disease\")\n",
    "    print(f\"\\n===== Disease : {disease} =====\")\n",
    "    print(subt.round(6))        # arrondi pour lisibilité\n",
    "    subt.to_csv(os.path.join(ANALYSIS_FOLDER,'DIFF', f\"diff_MAE_{disease}.csv\"))  # Save in the main folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────\n",
    "# 1. Charger le CSV\n",
    "# ──────────────────────────────────────────────\n",
    "df = gather_all_method_files(MAINFOLDER, robust_methods_for_analysis, \"train\")\n",
    "df = df[df[\"method\"] != \"robust_rwp\"]\n",
    "df = df[df[\"method\"] != \"raw\"]\n",
    "\n",
    "meta_cols   = [\"site\", \"method\", \"robust_method\", \"disease\", \"metric\"]\n",
    "bundle_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "# Colonnes méta vs. bundles\n",
    "meta_cols   = [\"site\", \"method\", \"robust_method\", \"disease\", \"metric\"]\n",
    "bundle_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 2. Passer en format « long »  → 1 ligne = 1 bundle\n",
    "# ──────────────────────────────────────────────\n",
    "long_df = (\n",
    "    df\n",
    "    .melt(id_vars=meta_cols, value_vars=bundle_cols,\n",
    "          var_name=\"bundle\", value_name=\"mae\")\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 3. Baseline  (NoRobust / No)  -> mae_base\n",
    "# ──────────────────────────────────────────────\n",
    "base_mask = (long_df[\"method\"] == \"NoRobust\") & (long_df[\"robust_method\"] == \"No\")\n",
    "base_df   = (\n",
    "    long_df[base_mask]\n",
    "    .rename(columns={\"mae\": \"mae_base\"})\n",
    "    .loc[:, [\"disease\", \"site\", \"metric\", \"bundle\", \"mae_base\"]]\n",
    ")\n",
    "\n",
    "# Fusionner pour ajouter la colonne mae_base\n",
    "long_df = long_df.merge(base_df,\n",
    "                        on=[\"disease\", \"site\", \"metric\", \"bundle\"],\n",
    "                        how=\"left\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 4. Différence bundle-par-bundle\n",
    "# ──────────────────────────────────────────────\n",
    "long_df[\"diff\"] = long_df[\"mae\"] - long_df[\"mae_base\"]\n",
    "\n",
    "# Étiquette de méthode : « hc » ou le robust_method\n",
    "def label(r):\n",
    "    return \"hc\" if (r[\"method\"] == \"hc\" and r[\"robust_method\"] == \"No\") else r[\"robust_method\"]\n",
    "\n",
    "long_df[\"method_label\"] = long_df.apply(label, axis=1)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 5. Tableau final  (index = site-metric-bundle, colonnes = méthode)\n",
    "# ──────────────────────────────────────────────\n",
    "pivot = (\n",
    "    long_df\n",
    "    .pivot_table(index=[\"disease\", \"site\", \"metric\", \"bundle\"],\n",
    "                 columns=\"method_label\",\n",
    "                 values=\"diff\",\n",
    "                 aggfunc=\"mean\")          # si jamais il y a des doublons\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Mettre « hc » en premier si présent\n",
    "cols = [\"hc\"] + [c for c in pivot.columns if c != \"hc\"]\n",
    "pivot = pivot[cols]\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 6. Afficher / sauver\n",
    "# ──────────────────────────────────────────────\n",
    "for disease in pivot.index.get_level_values(\"disease\").unique():\n",
    "    sub = pivot.xs(disease, level=\"disease\")\n",
    "    print(f\"\\n===== Disease : {disease} =====\")\n",
    "    print(sub.round(6))          # arrondi pour la lisibilité\n",
    "    sub.to_csv(os.path.join(ANALYSIS_FOLDER,'DIFF', f\"diff_MAE_per_bundle_{disease}.csv\"))  # Save in the main folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.formatting.rule import CellIsRule\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 1) Fonctions \"CSV → Styler\" et \"CSV → Excel\"\n",
    "# ──────────────────────────────────────────────\n",
    "def style_diff_csv(csv_path: str, n_index_cols: int = 3, tol: float = 0.0):\n",
    "    \"\"\"\n",
    "    Charge un CSV pivoté et renvoie un Styler coloré (affichage Jupyter).\n",
    "    n_index_cols = nb de colonnes d'index (ex.: site, metric, bundle = 3).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, index_col=list(range(n_index_cols)))\n",
    "    return _style_diff_df(df, tol)\n",
    "\n",
    "def excel_diff_csv(csv_path: str, excel_path: str,\n",
    "                   n_index_cols: int = 3, tol: float = 0.0):\n",
    "    \"\"\"\n",
    "    Charge un CSV pivoté et écrit un Excel coloré.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, index_col=list(range(n_index_cols)))\n",
    "    _excel_diff_df(df, excel_path, tol)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 2) Implémentations \"bas niveau\" (utilisées ci-dessus)\n",
    "# ──────────────────────────────────────────────\n",
    "def _style_diff_df(df: pd.DataFrame, tol: float):\n",
    "    hc_present = 'hc' in df.columns\n",
    "\n",
    "    def _row_style(row):\n",
    "        hc_val = row['hc'] if hc_present else np.nan\n",
    "        out = []\n",
    "        for v in row:\n",
    "            if pd.isna(v):\n",
    "                out.append('')\n",
    "            elif v > 0:\n",
    "                out.append('background-color:#ffcccc')\n",
    "            elif hc_present and v <= hc_val + tol:\n",
    "                out.append('background-color:#ccffcc')\n",
    "            else:\n",
    "                out.append('')\n",
    "        return out\n",
    "\n",
    "    return df.style.apply(_row_style, axis=1)\n",
    "\n",
    "def _excel_diff_df(df: pd.DataFrame, path: str, tol: float):\n",
    "    with pd.ExcelWriter(path, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='diff')\n",
    "        ws = writer.sheets['diff']\n",
    "\n",
    "        nrows = df.shape[0] + 1                # + entête\n",
    "        nindex = len(df.index.names)\n",
    "        first_col = 2 + nindex\n",
    "        last_col  = first_col + df.shape[1] - 1\n",
    "\n",
    "        red   = PatternFill(start_color=\"FFFFCCCC\", end_color=\"FFFFCCCC\", fill_type=\"solid\")\n",
    "        green = PatternFill(start_color=\"FFCCFFCC\", end_color=\"FFCCFFCC\", fill_type=\"solid\")\n",
    "\n",
    "        # Rouge: valeur > 0\n",
    "        for col in range(first_col, last_col + 1):\n",
    "            letter = get_column_letter(col)\n",
    "            ws.conditional_formatting.add(\n",
    "                f\"{letter}2:{letter}{nrows}\",\n",
    "                CellIsRule(operator='greaterThan', formula=['0'], fill=red)\n",
    "            )\n",
    "\n",
    "        # Vert: valeur ≤ hc + tol\n",
    "        if 'hc' in df.columns:\n",
    "            hc_idx = list(df.columns).index('hc')\n",
    "            hc_letter = get_column_letter(first_col + hc_idx)\n",
    "            for col in range(first_col, last_col + 1):\n",
    "                letter = get_column_letter(col)\n",
    "                formula = [f\"{letter}2<={hc_letter}2+{tol}\"]\n",
    "                ws.conditional_formatting.add(\n",
    "                    f\"{letter}2:{letter}{nrows}\",\n",
    "                    CellIsRule(operator='lessThanOrEqual', formula=formula, fill=green)\n",
    "                )\n",
    "    print(f\"Excel enregistré : {path}\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 3) Exemple d’utilisation\n",
    "# ──────────────────────────────────────────────\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the files generated in the last two cells\n",
    "diff_mae_files = [\n",
    "    os.path.join(ANALYSIS_FOLDER,'DIFF', f\"diff_MAE_{disease}.csv\")\n",
    "    for disease in diseases\n",
    "]\n",
    "\n",
    "diff_mae_per_bundle_files = [\n",
    "    os.path.join(ANALYSIS_FOLDER,'DIFF',f\"diff_MAE_per_bundle_{disease}.csv\")\n",
    "    for disease in pivot.index.get_level_values(\"disease\").unique()\n",
    "]\n",
    "\n",
    "# Combine the lists of files\n",
    "generated_files = diff_mae_files + diff_mae_per_bundle_files\n",
    "\n",
    "# Print the list of generated files\n",
    "print(\"Generated files:\")\n",
    "for file in generated_files:\n",
    "    print(file)\n",
    "    styler = style_diff_csv(file, n_index_cols=3, tol=1e-4)\n",
    "    excel_file = file.replace(\".csv\", \"_colored.xlsx\")\n",
    "    excel_diff_csv(file, excel_file, n_index_cols=3, tol=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "\n",
    "# ───────────────────────────────────────────\n",
    "# 2. Helpers\n",
    "# ───────────────────────────────────────────\n",
    "\n",
    "def _ensure(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "\n",
    "def heatmap_auto(df, methods, out_dir, rows_per_chunk=250):\n",
    "    \"\"\"Crée une ou plusieurs heatmaps robustes.\"\"\"\n",
    "    pivot = df.set_index(\"bundle\")[methods].sort_index()\n",
    "    n_rows = len(pivot)\n",
    "    n_chunks = math.ceil(n_rows / rows_per_chunk)\n",
    "    for i in range(n_chunks):\n",
    "        chunk = pivot.iloc[i*rows_per_chunk:(i+1)*rows_per_chunk]\n",
    "        h = max(4, len(chunk)*0.2)\n",
    "        plt.figure(figsize=(12, h))\n",
    "        sns.heatmap(chunk, cmap=\"RdYlGn_r\", center=0, linewidths=.1)\n",
    "        plt.title(f\"Δ MAE – bundles × méthodes (part {i+1}/{n_chunks})\")\n",
    "        fname = f\"heatmap_part_{i+1:02d}.png\" if n_chunks > 1 else \"heatmap.png\"\n",
    "        plt.savefig(os.path.join(out_dir, fname), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def boxplot_methods(df, methods, out_dir):\n",
    "    melted = df.melt(id_vars=[\"bundle\"], value_vars=methods,\n",
    "                     var_name=\"method\", value_name=\"diff_mae\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.boxplot(data=melted, x=\"method\", y=\"diff_mae\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.axhline(0, ls=\"--\", c=\"grey\")\n",
    "    plt.title(\"Distribution des Δ MAE par méthode\")\n",
    "    plt.savefig(os.path.join(out_dir, \"boxplot_methods.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def barchart_mean(df, methods, out_dir):\n",
    "    means = df[methods].mean().sort_values()\n",
    "    means.plot(kind=\"barh\", figsize=(6, max(4, len(means)*.4)))\n",
    "    plt.axvline(0, ls=\"--\", c=\"grey\")\n",
    "    plt.xlabel(\"Δ MAE moyen (négatif = mieux)\")\n",
    "    plt.title(\"Performance moyenne par méthode\")\n",
    "    plt.savefig(os.path.join(out_dir, \"barchart_mean.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def radars_per_site(df, methods, out_dir):\n",
    "    sites = df[\"site\"].unique()\n",
    "    n = len(methods)\n",
    "    angles = np.linspace(0, 2*np.pi, n, endpoint=False)\n",
    "    for s in sites:\n",
    "        stats = df[df[\"site\"] == s][methods].mean()\n",
    "        vals = np.concatenate([stats.values, stats.values[:1]])\n",
    "        angs = np.append(angles, angles[0])\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        ax = fig.add_subplot(111, polar=True)\n",
    "        ax.plot(angs, vals, marker=\"o\")\n",
    "        ax.fill(angs, vals, alpha=.25)\n",
    "        ax.set_xticks(angles)\n",
    "        ax.set_xticklabels(methods, size=7)\n",
    "        ax.set_title(f\"Profil Δ MAE – {s}\")\n",
    "        plt.savefig(os.path.join(out_dir, f\"radar_{s}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def scatters_raw_vs_method(df, methods, out_dir):\n",
    "    base = \"hc\"\n",
    "    for m in methods:\n",
    "        if m in (\"hc\", base):\n",
    "            continue\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.scatter(df[base], df[m], alpha=.5, s=15)\n",
    "        plt.axhline(0, ls=\"--\", c=\"grey\")\n",
    "        plt.axvline(0, ls=\"--\", c=\"grey\")\n",
    "        plt.xlabel(\"Δ MAE raw\")\n",
    "        plt.ylabel(f\"Δ MAE {m}\")\n",
    "        plt.title(f\"raw vs {m}\")\n",
    "        plt.savefig(os.path.join(out_dir, f\"scatter_raw_vs_{m}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "# ───────────────────────────────────────────\n",
    "# 3. Exécution multi‑diseases\n",
    "# ───────────────────────────────────────────\n",
    "autosplit_rows  = 250\n",
    "for disease in diseases:\n",
    "    csv_path = os.path.join(ANALYSIS_FOLDER, \"DIFF\", f\"diff_MAE_per_bundle_{disease}.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"⚠️  CSV introuvable :\", csv_path)\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    methods = [c for c in df.columns if c not in (\"site\", \"metric\", \"bundle\")]\n",
    "\n",
    "    root = os.path.join(ANALYSIS_FOLDER, \"VIS_OUTPUT\", disease)\n",
    "    heat_dir   = _ensure(os.path.join(root, \"heatmaps\"))\n",
    "    box_dir    = _ensure(os.path.join(root, \"boxplots\"))\n",
    "    bar_dir    = _ensure(os.path.join(root, \"bar_charts\"))\n",
    "    radar_dir  = _ensure(os.path.join(root, \"radars\"))\n",
    "    scatter_dir= _ensure(os.path.join(root, \"scatters\"))\n",
    "\n",
    "    # heatmap_auto(df, methods, heat_dir, rows_per_chunk=autosplit_rows)\n",
    "    boxplot_methods(df, methods, box_dir)\n",
    "    barchart_mean(df, methods, bar_dir)\n",
    "    radars_per_site(df, methods, radar_dir)\n",
    "    scatters_raw_vs_method(df, methods, scatter_dir)\n",
    "\n",
    "    print(f\"✅ {disease} → graphes dans {os.path.abspath(root)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOP_N = 5   \n",
    "\n",
    "for disease in diseases:\n",
    "    csv_path = os.path.join(ANALYSIS_FOLDER, \"DIFF\", f\"diff_MAE_per_bundle_{disease}.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"⚠️  CSV introuvable :\", csv_path)\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    methods = [c for c in df.columns if c not in (\"site\", \"metric\", \"bundle\")]\n",
    "\n",
    "    # Reshape longue pour simplifier l'extraction\n",
    "    long = df.melt(id_vars=[\"site\", \"metric\", \"bundle\"], value_vars=methods,\n",
    "                   var_name=\"method\", value_name=\"diff_mae\")\n",
    "\n",
    "    # Garder seulement les cas où diff_mae est non‑nul / non‑NaN\n",
    "    long = long.dropna(subset=[\"diff_mae\"])\n",
    "\n",
    "    worst_list = []\n",
    "    for m in methods:\n",
    "        top_bad = long[long[\"method\"] == m].nlargest(TOP_N, \"diff_mae\")\n",
    "        worst_list.append(top_bad)\n",
    "\n",
    "    worst_df = pd.concat(worst_list, ignore_index=True)\n",
    "\n",
    "    # Tri final pour lisibilité: par méthode puis diff_mae décroissant\n",
    "    worst_df = worst_df.sort_values([\"method\", \"diff_mae\"], ascending=[True, False])\n",
    "\n",
    "    out_csv = os.path.join(ANALYSIS_FOLDER, f\"{disease}_worst_cases_top{TOP_N}.csv\")\n",
    "    worst_df.to_csv(out_csv, index=False)\n",
    "\n",
    "    print(f\"✅ {disease}: CSV des {TOP_N} pires cas par méthode → {out_csv}\")\n",
    "    display_cols = [\"method\", \"diff_mae\", \"site\", \"metric\", \"bundle\"]\n",
    "    print(worst_df[display_cols])  # aperçu rapide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # TEST ADD BIAIS\n",
    "# # Split the data into training and testing sets\n",
    "# directory = os.path.join(MAINFOLDER, \"testBiais\")\n",
    "# os.makedirs(directory, exist_ok=True)\n",
    "# train_df, test_df = split_train_test(CAMCAN, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Generate biased data\n",
    "# # Save the original non-biased data to temporary files\n",
    "# temp_train_file_original = os.path.join(directory, \"temp_train_original.csv\")\n",
    "# temp_test_file_original = os.path.join(directory, \"temp_test_original.csv\")\n",
    "# train_df.to_csv(temp_train_file_original, index=False)\n",
    "# test_df.to_csv(temp_test_file_original, index=False)\n",
    "\n",
    "# # Generate biased data\n",
    "# sampled_df_biaied, test_df_biaised, gammas,deltas, ruffles= generate_biaised_data(train_df, test_df)\n",
    "\n",
    "# # Save the biased data to temporary files\n",
    "# temp_train_file = os.path.join(directory, \"temp_train_biased.csv\")\n",
    "# temp_test_file = os.path.join(directory, \"temp_test_biased.csv\")\n",
    "# sampled_df_biaied.to_csv(temp_train_file, index=False)\n",
    "# test_df_biaised.to_csv(temp_test_file, index=False)\n",
    "\n",
    "# # Run the combat_visualize_data script\n",
    "# outname_train = os.path.join(\"visualize_train\")\n",
    "# cmd = (\n",
    "#     \"scripts/combat_visualize_data.py\"\n",
    "#     + \" \"\n",
    "#     + temp_train_file_original\n",
    "#     + \" \"\n",
    "#     + temp_train_file\n",
    "#     + \" --out_dir \"\n",
    "#     + directory\n",
    "#     + \" --outname \"\n",
    "#     + outname_train\n",
    "#     + \" -f\"\n",
    "#     + \" --bundles all\"\n",
    "# )\n",
    "# subprocess.call(cmd, shell=True)\n",
    "\n",
    "# # Display gammas and deltas along with their mean and standard deviation\n",
    "# print(\"Gammas:\", gammas)\n",
    "# print(\"Deltas:\", deltas)\n",
    "# gammas = list(gammas.values())\n",
    "# deltas = list(deltas.values())\n",
    "# print(\"\\nGamma Statistics:\")\n",
    "# print(f\"Mean: {np.mean(gammas)}, Std: {np.std(gammas)}\")\n",
    "\n",
    "# print(\"\\nDelta Statistics:\")\n",
    "# print(f\"Mean: {np.mean(deltas)}, Std: {np.std(deltas)}\")\n",
    "# print(\"Ruffles:\", ruffles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST Powerpoint generation\n",
    "# d  = os.path.join(MAINFOLDER, robust_method, \"adni_100_Philips_3T\")\n",
    "# create_presentation(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST the sample_patients function with compilation data data\n",
    "# sampled_df = sample_patients(COMPILATION, num_patients=100, disease_ratio=0.5)\n",
    "# print(sampled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_metrics(\"ROBUST/IQR/50_30/0/\", \"50_patients_30_percent_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the dists_compilation and metrics_compilation CSV files\n",
    "# dists_compilation_path = os.path.join(directory, \"dists_compilation.csv\")\n",
    "# metrics_compilation_path = os.path.join(directory, \"metrics_compilation.csv\")\n",
    "\n",
    "# dists_compilation = pd.read_csv(dists_compilation_path)\n",
    "# metrics_compilation = pd.read_csv(metrics_compilation_path)\n",
    "\n",
    "# # Change the site column\n",
    "# dists_compilation['site'] = dists_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "# metrics_compilation['site'] = metrics_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# # Display the means by site\n",
    "# dists_means_by_site = dists_compilation.groupby(['site','comparaison']).mean()\n",
    "# metrics_means_by_site = metrics_compilation.groupby('site').mean()\n",
    "\n",
    "# print(dists_means_by_site)\n",
    "# print(metrics_means_by_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIX METRICS COMPILATION\n",
    "# directory = os.path.join(MAINFOLDER, robust_method)\n",
    "# df = pd.read_csv(os.path.join(directory, \"metrics_compilation.csv\"))\n",
    "\n",
    "# # Group by the site\n",
    "# grouped = df.groupby('site')\n",
    "\n",
    "# # Process each site\n",
    "# cleaned_dfs = []\n",
    "# for site, group in grouped:\n",
    "#     # Reset index for easier manipulation\n",
    "#     group = group.reset_index(drop=True)\n",
    "    \n",
    "#     # # The first row is the \"bundle row\" (new column names)\n",
    "#     # new_columns = group.iloc[0].values  # Extract column names from the first row\n",
    "#     # new_columns[-1] = 'site'\n",
    "#     # group = group.iloc[1:]  # Remove the first row\n",
    "    \n",
    "#     # # Assign new column names\n",
    "#     # group.columns = new_columns\n",
    "    \n",
    "#     # # Sort the columns alphabetically (excluding 'site')\n",
    "#     # sorted = group.sort_index(axis=1)\n",
    "#     # Add a new column 'nomm' with the value indicating the metric for each row\n",
    "#     metrics = ['tp', 'fp', 'tn', 'fn', 'precision', 'recall', 'taux_faux_positifs', 'f1_score']\n",
    "#     group['metric'] = metrics\n",
    "    \n",
    "#     # # Append the cleaned DataFrame for this site\n",
    "#     cleaned_dfs.append(group)\n",
    "\n",
    "# # Concatenate all cleaned DataFrames\n",
    "# final_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "\n",
    "# # Save or display the result\n",
    "# final_df.to_csv(os.path.join(directory, \"metrics_compilation.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REAL SITES\n",
    "# directory = os.path.join(MAINFOLDER, robust_method)\n",
    "# raw_directory = os.path.join(RAWFOLDER, site_group)\n",
    "# for filename in sorted(os.listdir(raw_directory)):\n",
    "#     f = os.path.join(raw_directory, filename)\n",
    "#     # checking if it is a file\n",
    "#     if os.path.isfile(f):\n",
    "#         analyse_site(f, robust_method, directory)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
