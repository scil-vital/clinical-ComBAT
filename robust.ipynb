{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "import os\n",
    "\n",
    "from scripts import combat_info\n",
    "from scripts import combat_quick_apply\n",
    "from scripts import combat_quick_QC\n",
    "\n",
    "\n",
    "CAMCAN = \"./DONNES/CamCAN.md.raw.csv.gz\"\n",
    "COMPILATION = \"./DONNES/adni_compilation.csv.gz\"\n",
    "\n",
    "MAINFOLDER = \"ROBUST\"\n",
    "\n",
    "RAWFOLDER = \"RAW\"\n",
    "\n",
    "site_group = 'ADNI'\n",
    "robust_method = 'IQR'\n",
    "metric = \"md\"\n",
    "method= \"classic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(file_path, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into training and testing sets, ensuring the same proportion of HC and non-HC patients\n",
    "    and that data from the same sid are in the same dataset.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file to split.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Training set.\n",
    "    pd.DataFrame: Testing set.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Group by 'sid' and get unique sids\n",
    "    unique_sids = df.groupby('sid').first().reset_index()\n",
    "    \n",
    "    # Split the unique sids into train and test sets\n",
    "    train_sids, test_sids = train_test_split(unique_sids, test_size=test_size, random_state=random_state, stratify=unique_sids['disease'])\n",
    "    \n",
    "    # Create train and test DataFrames by filtering the original DataFrame\n",
    "    train_df = df[df['sid'].isin(train_sids['sid'])]\n",
    "    test_df = df[df['sid'].isin(test_sids['sid'])]\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_patients(df, num_patients, disease_ratio,index):\n",
    "    # Lire le fichier CSV dans un DataFrame\n",
    "    \n",
    "    # Calculer le nombre de patients malades et sains\n",
    "    num_diseased = int(num_patients * disease_ratio)\n",
    "    num_healthy = num_patients - num_diseased\n",
    "    \n",
    "    # Filtrer les patients en santé (HC) et malades\n",
    "    healthy_patients = df[df['disease'] == 'HC']\n",
    "    diseased_patients = df[df['disease'] != 'HC']\n",
    "    \n",
    "    # S'assurer qu'il y a assez de patients pour chaque catégorie\n",
    "    if len(healthy_patients['sid'].unique()) < num_healthy or len(diseased_patients['sid'].unique()) < num_diseased:\n",
    "        raise ValueError(\"Nombre insuffisant de patients en santé ou malades pour l'échantillon demandé.\")\n",
    "    \n",
    "    # Sélectionner un échantillon aléatoire de patients sains et malades\n",
    "    sampled_healthy = healthy_patients.groupby('sid').sample(frac=1).head(num_healthy * df['bundle'].nunique())\n",
    "    sampled_diseased = diseased_patients.groupby('sid').sample(frac=1).head(num_diseased * df['bundle'].nunique())\n",
    "    \n",
    "    # Combiner les échantillons pour obtenir le DataFrame final\n",
    "    sampled_df = pd.concat([sampled_healthy, sampled_diseased])\n",
    "    # Modifier les valeurs de 'site' pour toutes les lignes\n",
    "    sampled_df['site'] = f\"{num_patients}_patients_{int(disease_ratio*100)}_percent_{index}\"\n",
    "    \n",
    "    # Retourner le DataFrame final\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biaised_data(df1, df2, \n",
    "                additive_uniform_low=-4, additive_uniform_high=4, \n",
    "                multiplicative_uniform_low=0.5, multiplicative_uniform_high=2, \n",
    "                additive_std_low=0.01, additive_std_high=0.1, \n",
    "                multiplicative_std_low=0.01, multiplicative_std_high=0.1):\n",
    "    \"\"\"\n",
    "    Génère des biais additifs et multiplicatifs pour chaque bundle en fonction de df1, puis applique ces biais à df1 et df2\n",
    "    de manière indépendante en tenant compte des covariables (âge, sexe, latéralité) et en centrant les résidus.\n",
    "\n",
    "    Parameters:\n",
    "    - df1, df2 (pd.DataFrame): Les DataFrames sur lesquels appliquer les biais.\n",
    "    - additive_uniform_low, additive_uniform_high : paramètres pour le biais additif.\n",
    "    - multiplicative_uniform_low, multiplicative_uniform_high : paramètres pour le biais multiplicatif.\n",
    "    - additive_std_low, additive_std_high : paramètres pour l'écart-type du biais additif.\n",
    "    - multiplicative_std_low, multiplicative_std_high : paramètres pour l'écart-type du biais multiplicatif.\n",
    "\n",
    "    Returns:\n",
    "    - tuple : Deux DataFrames avec les biais appliqués indépendamment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionnaires pour stocker les biais par bundle\n",
    "    additive_bias_per_bundle = {}\n",
    "    multiplicative_bias_per_bundle = {}\n",
    "\n",
    "    # # Tirer les moyennes de biais de distributions uniformes pour le bundle\n",
    "    additive_mean = np.random.uniform(low=additive_uniform_low, high=additive_uniform_high)\n",
    "    multiplicative_mean = np.random.uniform(low=multiplicative_uniform_low, high=multiplicative_uniform_high)\n",
    "    \n",
    "    # # Tirer les écarts-types de biais de distributions uniformes pour le bundle\n",
    "    additive_std = np.random.uniform(low=additive_std_low, high=additive_std_high)\n",
    "    multiplicative_std = np.random.uniform(low=multiplicative_std_low, high=multiplicative_std_high)\n",
    "\n",
    "    # Calcul des biais pour chaque bundle unique dans df1\n",
    "    for bundle in df1['bundle'].unique(): \n",
    "        # Générer un biais additif et multiplicatif spécifique au bundle\n",
    "        additive_bias_per_bundle[bundle] = np.random.normal(loc=additive_mean, scale=additive_std)\n",
    "        multiplicative_bias_per_bundle[bundle] = np.random.normal(loc=multiplicative_mean, scale=multiplicative_std)\n",
    "   \n",
    "    # Appliquer les biais indépendamment à df1 et df2 en utilisant les mêmes biais générés\n",
    "    biased_df1 = apply_bias(df1, additive_bias_per_bundle, multiplicative_bias_per_bundle)\n",
    "    biased_df2 = apply_bias(df2, additive_bias_per_bundle, multiplicative_bias_per_bundle)\n",
    "    \n",
    "    return biased_df1, biased_df2, additive_bias_per_bundle, multiplicative_bias_per_bundle, [additive_mean, multiplicative_mean, additive_std, multiplicative_std]\n",
    "\n",
    "def apply_bias(dataframe, additive_bias_per_bundle, multiplicative_bias_per_bundle):\n",
    "    biased_df = dataframe.copy()\n",
    "    biased_means = []\n",
    "    \n",
    "    # Application de la régression et des biais pour chaque bundle unique\n",
    "    for bundle in biased_df['bundle'].unique():\n",
    "        # Filtrer le DataFrame pour le bundle actuel\n",
    "        bundle_df = biased_df[biased_df['bundle'] == bundle]\n",
    "        \n",
    "        # Préparer les covariables pour la régression\n",
    "        X = bundle_df[['age', 'sex', 'handedness']]\n",
    "        y = bundle_df['mean']\n",
    "        \n",
    "        # Ajuster le modèle de régression linéaire pour le bundle\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Calculer les prédictions et les résidus pour le bundle\n",
    "        predicted_mean = model.predict(X)\n",
    "        residuals = y - predicted_mean\n",
    "\n",
    "        # Récupérer les biais pour le bundle actuel\n",
    "        additive_bias = additive_bias_per_bundle[bundle]\n",
    "        multiplicative_bias = multiplicative_bias_per_bundle[bundle]\n",
    "        \n",
    "        # Appliquer les biais aux résidus centrés et réintégrer les effets des covariables\n",
    "        biased_means_bundle = residuals * multiplicative_bias + additive_bias * np.std(y) + predicted_mean\n",
    "        biased_means.extend(biased_means_bundle)\n",
    "    \n",
    "    # Assigner les valeurs biaisées calculées au DataFrame\n",
    "    biased_df['mean'] = biased_means\n",
    "    return biased_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(mov_data_file):\n",
    "    [df,bundles] = combat_info.info(mov_data_file)\n",
    "    nb_hc = int(re.findall('HC\\(n=(\\d+)',df[\"DetailInfos\"][\"Disease\"])[0])\n",
    "    nb_total = df[\"DetailInfos\"][\"Number of Subject\"]\n",
    "    nb_sick = nb_total - nb_hc\n",
    "    return [nb_total,nb_hc,nb_sick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundles(mov_data_file):\n",
    "    return combat_info.get_bundles(mov_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_text(x):\n",
    "    return \"NoRobust\" if x == 'No' else x\n",
    "\n",
    "def rwp_text(x):\n",
    "    return \"RWP\" if x else \"NoRWP\"\n",
    "def get_site(mov_data_file):\n",
    "    mov_data = pd.read_csv(mov_data_file)\n",
    "    return mov_data.site.unique()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(mov_data_file, robust, rwp, directory, hc,):\n",
    "    ###########\n",
    "    ### fit ###\n",
    "    ###########\n",
    "    output_model_filename = (\n",
    "            get_site(mov_data_file)\n",
    "            + \".\"\n",
    "            + metric\n",
    "            + \".\"\n",
    "            + method\n",
    "            + \".\"\n",
    "            + robust_text(robust)\n",
    "            + \".\"\n",
    "            + rwp_text(rwp)\n",
    "            + \".model.csv\"\n",
    "        )\n",
    "    cmd = (\n",
    "        \"scripts/combat_quick_fit.py\"\n",
    "        + \" \"\n",
    "        + CAMCAN\n",
    "        + \" \"\n",
    "        + mov_data_file\n",
    "        + \" --out_dir \"\n",
    "        + directory\n",
    "        + \" --output_model_filename \"\n",
    "        + output_model_filename\n",
    "        + \" --method \"\n",
    "        + method\n",
    "        + \" --robust \"\n",
    "        + robust\n",
    "        + \" -f \"\n",
    "    )\n",
    "    if rwp:\n",
    "        cmd += ' --rwp'\n",
    "    if hc: \n",
    "        cmd += ' --hc'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    return output_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(mov_data_file, model_filename, robust, rwp, directory):\n",
    "    output_filename = os.path.join(\n",
    "            directory,\n",
    "            get_site(mov_data_file)\n",
    "            + \".\"\n",
    "            + metric\n",
    "            + \".\"\n",
    "            + method\n",
    "            + \".\"\n",
    "            + robust_text(robust)\n",
    "            + \".\"\n",
    "            + rwp_text(rwp)\n",
    "            + \".csv\"\n",
    "        )\n",
    "    return output_filename, combat_quick_apply.apply(mov_data_file, model_filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_harmonization(f, new_f, directory):\n",
    "    cmd = (\n",
    "        \"scripts/combat_visualize_harmonization.py\"\n",
    "        + \" \"\n",
    "        + CAMCAN\n",
    "        + \" \"\n",
    "        + f\n",
    "        + \" \"\n",
    "        + new_f\n",
    "        + \" --out_dir \"\n",
    "        + directory\n",
    "        #+ \" --bundles all\"\n",
    "        + \" -f\"\n",
    "    )\n",
    "    subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QC(output_filename, output_model_filename):\n",
    "    return combat_quick_QC.QC(CAMCAN,output_filename, output_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_presentation(directory):\n",
    "    # Create a presentation object\n",
    "    prs = Presentation()\n",
    "    \n",
    "    # Define the subdirectories\n",
    "    subdirs = [\"hc\", \"NoRobust\", \"robust\", \"robust_rwp\"]\n",
    "    # Get the list of images\n",
    "    images = [img for img in os.listdir(os.path.join(directory, subdirs[0])) if method in img and img.endswith('.png')]\n",
    "    \n",
    "    for img in images:\n",
    "        slide_layout = prs.slide_layouts[5]  # Use a blank slide layout\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        \n",
    "        for i, subdir in enumerate(subdirs):\n",
    "            img_path = os.path.join(directory, subdir, img)\n",
    "            left = Inches(0.5 + (i % 2) * 4.5)  # Positioning images in two columns\n",
    "            top = Inches(0.2 + (i // 2) * 3.5)  # Positioning images in two rows with more space between rows\n",
    "            \n",
    "            # Add text above the image\n",
    "            text_box = slide.shapes.add_textbox(left, top, width=Inches(4), height=Inches(0.5))\n",
    "            text_frame = text_box.text_frame\n",
    "            text_frame.text = subdir\n",
    "            \n",
    "            # Add the image\n",
    "            slide.shapes.add_picture(img_path, left, top + Inches(0.5), width=Inches(4))\n",
    "    \n",
    "    # Save the presentation\n",
    "    prs.save(os.path.join(directory, 'harmonization_results.pptx'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distances(directory, site, hc_dists, no_robust_dists, robust_dists, robust_rwp_dists):\n",
    "    comparison_results = {\n",
    "        \"hc_vs_no_robust\": (np.array(hc_dists) - np.array(no_robust_dists))/np.array(no_robust_dists)*100,\n",
    "        \"robust_vs_no_robust\": (np.array(robust_dists) - np.array(no_robust_dists))/np.array(no_robust_dists)*100,\n",
    "        \"robust_rwp_vs_no_robust\": (np.array(robust_rwp_dists) - np.array(no_robust_dists))/np.array(no_robust_dists)*100\n",
    "    }\n",
    "    df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    # Calculer le nombre de comparaisons négatives et positives, et les moyennes et médianes\n",
    "    results = []\n",
    "    for method in comparison_results.keys():\n",
    "        negative_values = df[method][df[method] < 0]\n",
    "        positive_values = df[method][df[method] >= 0]\n",
    "        \n",
    "        num_negative = len(negative_values)\n",
    "        num_positive = len(positive_values)\n",
    "        \n",
    "        mean_negative = negative_values.mean() if num_negative > 0 else 0\n",
    "        mean_positive = positive_values.mean() if num_positive > 0 else 0\n",
    "        \n",
    "        median_negative = negative_values.median() if num_negative > 0 else 0\n",
    "        median_positive = positive_values.median() if num_positive > 0 else 0\n",
    "        \n",
    "        mean_difference = df[method].mean()\n",
    "        \n",
    "        results.append({\n",
    "            \"site\": site,\n",
    "            \"comparaison\": method,\n",
    "            \"Nb comp. nég.\": num_negative,\n",
    "            \"Nb comp. pos.\": num_positive,\n",
    "            \"Moy. tot.\": mean_difference,\n",
    "            \"Moy. val. nég.\": mean_negative,\n",
    "            \"Moy. val. pos.\": mean_positive,\n",
    "            \"Méd. val. nég.\": median_negative,\n",
    "            \"Méd. val. pos.\": median_positive\n",
    "        })\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(directory, f\"{site}_comparison_results.csv\"), index=False)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize(f_train, f_test, directory, robust, rwp,hc):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f_train)\n",
    "    \n",
    "    # Fit the model\n",
    "    output_model_filename = fit(f_train, robust, rwp, directory, hc)\n",
    "    output_model_filename = os.path.join(directory, output_model_filename)\n",
    "    # Apply the model\n",
    "    output_filename, y_harm = apply(f_test, output_model_filename, robust, rwp, directory) \n",
    "    \n",
    "    # Perform quality control\n",
    "    dists = QC(output_filename, output_model_filename)\n",
    "    \n",
    "    # Visualize the harmonization\n",
    "    visualize_harmonization(f_test, output_filename, directory)\n",
    "    \n",
    "    # If robust is not \"No\", load metrics and outliers\n",
    "    if robust != \"No\":\n",
    "        metrics_filename = os.path.join(directory, f\"metrics_{get_site(f_train)}_{robust_text(robust)}_{rwp_text(rwp)}.csv\")\n",
    "        outliers_filename = os.path.join(directory, f\"outliers_{get_site(f_train)}_{robust_text(robust)}_{rwp_text(rwp)}.csv\")\n",
    "        \n",
    "        # Load metrics from CSV file\n",
    "        loaded_metrics = pd.read_csv(metrics_filename, index_col=0)\n",
    "        \n",
    "        # Load outliers from CSV file\n",
    "        loaded_outliers_df = pd.read_csv(outliers_filename, index_col=0)\n",
    "        \n",
    "        return [dists, loaded_metrics, loaded_outliers_df]\n",
    "    return[dists, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_site(f_train,f_test, robust, directory):\n",
    "    # 4 harmonization\n",
    "    harmonization_hc = harmonize(f_train, f_test, os.path.join(directory, \"hc\"), \"No\", False, True)\n",
    "    harmonization_no_robust = harmonize(f_train, f_test, os.path.join(directory, \"NoRobust\"), \"No\", False, False)\n",
    "    harmonization_robust = harmonize(f_train, f_test, os.path.join(directory, \"robust\"), robust, False, False)\n",
    "    harmonization_robust_rwp = harmonize(f_train, f_test, os.path.join(directory, \"robust_rwp\"), robust, True, False)\n",
    "\n",
    "\n",
    "    create_presentation(directory)\n",
    "\n",
    "    dists_analyze = compare_distances(directory, get_site(f_train), harmonization_hc[0], harmonization_no_robust[0], harmonization_robust[0], harmonization_robust_rwp[0])\n",
    "    \n",
    "    #TODO bundles et analyze outliers\n",
    "    return dists_analyze, harmonization_robust[1], harmonization_robust[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Analyse Method\n",
    "# #sample_sizes = [5,10,30, 50, 100, 150, 200,300]  # Différentes tailles d'échantillon\n",
    "# #disease_ratios = [0.05, 0.1, 0.3, 0.5, 0.7]  # Différents pourcentages de malades\n",
    "# sample_sizes = [30, 50, 150]  # Différentes tailles d'échantillon\n",
    "# disease_ratios = [0.1, 0.3, 0.5]  # Différents pourcentages de malades\n",
    "# num_tests = 2  # Nombre de tests à effectuer pour chaque combinaison\n",
    "# # Split the data into training and testing sets\n",
    "# directory = os.path.join(MAINFOLDER, robust_method)\n",
    "# train_df, test_df = split_train_test(COMPILATION, test_size=0.2, random_state=42)\n",
    "# # Initialize DataFrames to store the results\n",
    "# metrics_compilation = pd.DataFrame()\n",
    "# dists_compilation = pd.DataFrame()\n",
    "# outliers_compilation = pd.DataFrame()\n",
    "# for sample_size in sample_sizes:\n",
    "#     for disease_ratio in disease_ratios:\n",
    "        \n",
    "#         sizeDir = os.path.join(directory, f\"{sample_size}_{int(disease_ratio*100)}\")\n",
    "#         for i in range(num_tests):\n",
    "#             tempDir = os.path.join(sizeDir, f\"{i}\")\n",
    "#             os.makedirs(tempDir, exist_ok=True)\n",
    "            \n",
    "#             # Sauvegarder l'échantillon dans un fichier temporaire\n",
    "#             temp_file = os.path.join(tempDir, f\"train_{sample_size}_{int(disease_ratio*100)}_{i}.csv\")\n",
    "\n",
    "#             test_file = os.path.join(tempDir, f\"test_{sample_size}_{int(disease_ratio*100)}_{i}.csv\")\n",
    "            \n",
    "#             # Analyser le site pour le nouvel échantillon\n",
    "#             dists_analyze, metrics, outliers = analyse_site(temp_file, test_file, robust_method, tempDir)\n",
    "#             metrics_compilation = pd.concat([metrics_compilation, metrics])\n",
    "#             dists_compilation = pd.concat([dists_compilation, dists_analyze])\n",
    "#             outliers_compilation = pd.concat([outliers_compilation, outliers])\n",
    "# # Save the metrics and distances compilation DataFrames to CSV files\n",
    "# metrics_compilation.to_csv(os.path.join(directory, \"metrics_compilation.csv\"), index=False)\n",
    "# dists_compilation.to_csv(os.path.join(directory, \"dists_compilation.csv\"), index=False)\n",
    "# outliers_compilation.to_csv(os.path.join(directory, \"outliers_compilation.csv\"), index=False)\n",
    "\n",
    "# dists_compilation['site'] = dists_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "# metrics_compilation['site'] = metrics_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# # Display the means by site\n",
    "# dists_means_by_site = dists_compilation.groupby(['site','comparaison']).mean()\n",
    "# metrics_means_by_site = metrics_compilation.groupby('site').mean()\n",
    "\n",
    "# metrics_means_by_site.to_csv(os.path.join(directory, \"metrics_compilation_mean.csv\"), index=False)\n",
    "# dists_means_by_site.to_csv(os.path.join(directory, \"dists_compilation_mean.csv\"), index=False)\n",
    "# print(\"FINI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REAL SITES\n",
    "# directory = os.path.join(MAINFOLDER, robust_method)\n",
    "# raw_directory = os.path.join(RAWFOLDER, site_group)\n",
    "# for filename in sorted(os.listdir(raw_directory)):\n",
    "#     f = os.path.join(raw_directory, filename)\n",
    "#     # checking if it is a file\n",
    "#     if os.path.isfile(f):\n",
    "#         analyse_site(f, robust_method, directory)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gammas: {'left_ventricle': -3.647586673874433, 'mni_AC': -3.6165479682646215, 'mni_AF_L': -3.679153972284244, 'mni_AF_R': -3.63622851447761, 'mni_AST_L': -3.6254036629122974, 'mni_AST_R': -3.555941602597745, 'mni_CC': -3.6465103682823976, 'mni_CCMid': -3.602065754135561, 'mni_CC_ForcepsMajor': -3.6650867394526063, 'mni_CC_ForcepsMinor': -3.7078917055163414, 'mni_CST_L': -3.5924880934879346, 'mni_CST_R': -3.6302430513318495, 'mni_C_L': -3.568313766823324, 'mni_C_R': -3.615818267950327, 'mni_FPT_L': -3.585697641968588, 'mni_FPT_R': -3.675533438298187, 'mni_F_L_R': -3.4952840217128833, 'mni_ICP_L': -3.7657620146791646, 'mni_ICP_R': -3.6382685858078934, 'mni_IFOF_L': -3.5668608652733322, 'mni_IFOF_R': -3.5645614130273118, 'mni_IIT_mask_skeletonFA': -3.549896306772501, 'mni_ILF_L': -3.6785982198609317, 'mni_ILF_R': -3.413291888446699, 'mni_MCP': -3.587989434808899, 'mni_ML_L': -3.571340541019628, 'mni_ML_R': -3.590313596084942, 'mni_MdLF_L': -3.563443611847447, 'mni_MdLF_R': -3.6374861159858183, 'mni_OPT_L': -3.6326594689240252, 'mni_OPT_R': -3.645856368182148, 'mni_OR_L': -3.6371624863063046, 'mni_OR_R': -3.6620225032695712, 'mni_PPT_L': -3.6553551498577956, 'mni_PPT_R': -3.525079421226444, 'mni_SCP': -3.659284440545806, 'mni_SLF_L': -3.66322379515116, 'mni_SLF_R': -3.571946274036453, 'mni_STT_L': -3.546683802173023, 'mni_STT_R': -3.6138429492983213, 'mni_UF_L': -3.6528462560242825, 'mni_UF_R': -3.617697288760288, 'mni_VOF_L': -3.5558176982568104, 'mni_VOF_R': -3.6955898329152825, 'right_ventricle': -3.708257916982118}\n",
      "Deltas: {'left_ventricle': 1.0987985862857446, 'mni_AC': 1.0728503181293072, 'mni_AF_L': 1.1022843906957964, 'mni_AF_R': 1.0900905466164488, 'mni_AST_L': 1.0908885523362781, 'mni_AST_R': 1.1033017664142293, 'mni_CC': 1.101141907103203, 'mni_CCMid': 1.0986165643179293, 'mni_CC_ForcepsMajor': 1.1057939204044742, 'mni_CC_ForcepsMinor': 1.0769296183859982, 'mni_CST_L': 1.0850484882605829, 'mni_CST_R': 1.1037690648730212, 'mni_C_L': 1.1026894967127023, 'mni_C_R': 1.1002127222966906, 'mni_FPT_L': 1.1205249667018, 'mni_FPT_R': 1.1140263312558958, 'mni_F_L_R': 1.1041773484499864, 'mni_ICP_L': 1.065448881846961, 'mni_ICP_R': 1.1117021292702922, 'mni_IFOF_L': 1.0968568181632072, 'mni_IFOF_R': 1.064468467147726, 'mni_IIT_mask_skeletonFA': 1.1026044988127248, 'mni_ILF_L': 1.0594807020146368, 'mni_ILF_R': 1.121349016035609, 'mni_MCP': 1.0894962067485792, 'mni_ML_L': 1.0868453396807083, 'mni_ML_R': 1.0914148631250982, 'mni_MdLF_L': 1.0885097906336283, 'mni_MdLF_R': 1.1041501713115618, 'mni_OPT_L': 1.0771746366545423, 'mni_OPT_R': 1.0891910355907402, 'mni_OR_L': 1.076795024620826, 'mni_OR_R': 1.0677277879499025, 'mni_PPT_L': 1.0983476024540613, 'mni_PPT_R': 1.1052202068240007, 'mni_SCP': 1.0836876693505562, 'mni_SLF_L': 1.097095757178318, 'mni_SLF_R': 1.0708433576777916, 'mni_STT_L': 1.0945041808247684, 'mni_STT_R': 1.0985216179647992, 'mni_UF_L': 1.0992646479028199, 'mni_UF_R': 1.0904846597206495, 'mni_VOF_L': 1.089220342964923, 'mni_VOF_R': 1.0837359226232608, 'right_ventricle': 1.0873982477660094}\n",
      "\n",
      "Gamma Statistics:\n",
      "Mean: -3.615931855308786, Std: 0.06171495717100702\n",
      "\n",
      "Delta Statistics:\n",
      "Mean: 1.0925040927133065, Std: 0.014225273193413853\n",
      "Ruffles: [-3.5995684757035074, 1.0929974887928524, 0.06947299562229795, 0.014597149437592771]\n"
     ]
    }
   ],
   "source": [
    "# # TEST ADD BIAIS\n",
    "# Split the data into training and testing sets\n",
    "directory = os.path.join(MAINFOLDER, \"testBiais\")\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "train_df, test_df = split_train_test(CAMCAN, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate biased data\n",
    "# Save the original non-biased data to temporary files\n",
    "temp_train_file_original = os.path.join(directory, \"temp_train_original.csv\")\n",
    "temp_test_file_original = os.path.join(directory, \"temp_test_original.csv\")\n",
    "train_df.to_csv(temp_train_file_original, index=False)\n",
    "test_df.to_csv(temp_test_file_original, index=False)\n",
    "\n",
    "# Generate biased data\n",
    "sampled_df_biaied, test_df_biaised, gammas,deltas, ruffles= generate_biaised_data(train_df, test_df)\n",
    "\n",
    "# Save the biased data to temporary files\n",
    "temp_train_file = os.path.join(directory, \"temp_train_biased.csv\")\n",
    "temp_test_file = os.path.join(directory, \"temp_test_biased.csv\")\n",
    "sampled_df_biaied.to_csv(temp_train_file, index=False)\n",
    "test_df_biaised.to_csv(temp_test_file, index=False)\n",
    "\n",
    "# Run the combat_visualize_data script\n",
    "outname_train = os.path.join(\"visualize_train\")\n",
    "cmd = (\n",
    "    \"scripts/combat_visualize_data.py\"\n",
    "    + \" \"\n",
    "    + temp_train_file_original\n",
    "    + \" \"\n",
    "    + temp_train_file\n",
    "    + \" --out_dir \"\n",
    "    + directory\n",
    "    + \" --outname \"\n",
    "    + outname_train\n",
    "    + \" -f\"\n",
    "    + \" --bundles all\"\n",
    ")\n",
    "subprocess.call(cmd, shell=True)\n",
    "\n",
    "# Display gammas and deltas along with their mean and standard deviation\n",
    "print(\"Gammas:\", gammas)\n",
    "print(\"Deltas:\", deltas)\n",
    "gammas = list(gammas.values())\n",
    "deltas = list(deltas.values())\n",
    "print(\"\\nGamma Statistics:\")\n",
    "print(f\"Mean: {np.mean(gammas)}, Std: {np.std(gammas)}\")\n",
    "\n",
    "print(\"\\nDelta Statistics:\")\n",
    "print(f\"Mean: {np.mean(deltas)}, Std: {np.std(deltas)}\")\n",
    "print(\"Ruffles:\", ruffles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST Powerpoint generation\n",
    "# d  = os.path.join(MAINFOLDER, robust_method, \"adni_100_Philips_3T\")\n",
    "# create_presentation(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST the sample_patients function with compilation data data\n",
    "# sampled_df = sample_patients(COMPILATION, num_patients=100, disease_ratio=0.5)\n",
    "# print(sampled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_metrics(\"ROBUST/IQR/50_30/0/\", \"50_patients_30_percent_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the dists_compilation and metrics_compilation CSV files\n",
    "# dists_compilation_path = os.path.join(directory, \"dists_compilation.csv\")\n",
    "# metrics_compilation_path = os.path.join(directory, \"metrics_compilation.csv\")\n",
    "\n",
    "# dists_compilation = pd.read_csv(dists_compilation_path)\n",
    "# metrics_compilation = pd.read_csv(metrics_compilation_path)\n",
    "\n",
    "# # Change the site column\n",
    "# dists_compilation['site'] = dists_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "# metrics_compilation['site'] = metrics_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# # Display the means by site\n",
    "# dists_means_by_site = dists_compilation.groupby(['site','comparaison']).mean()\n",
    "# metrics_means_by_site = metrics_compilation.groupby('site').mean()\n",
    "\n",
    "# print(dists_means_by_site)\n",
    "# print(metrics_means_by_site)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
