{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Définir seuil méthode robuste\n",
        "\n",
        "Ce notebook explore différents seuils pour plusieurs méthodes robustes en réutilisant les jeux synthétiques «ALL».\\\n",
        "Il automatise l'évaluation (MAE) pour chaque seuil, puis génère les mêmes types de graphiques que l'analyse MAE :\\\n",
        "10 courbes par métrique et une moyenne globale par méthode.\n",
        "\n",
        "⚠️ Les boucles peuvent être coûteuses : ajustez les filtres (tests, ratios) avant d'exécuter l'évaluation complète.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from robust_evaluation_tools.robust_utils import (\n",
        "    get_metrics,\n",
        "    get_camcan_file,\n",
        "    format_param_token,\n",
        ")\n",
        "from robust_evaluation_tools.robust_harmonization import (\n",
        "    fit,\n",
        "    apply,\n",
        "    compare_with_compilation,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HARMONIZATION_METHOD = \"gmm\"\n",
        "\n",
        "PROCESSED_ROOT = Path(\"RESULTS/MAE_TEST/PROCESS_gmm/ALL\")\n",
        "SYNTHETIC_ROOT = Path(\"RESULTS/MAE_TEST/SYNTHETIC_SITES/v1/ALL\")\n",
        "OUTPUT_ROOT = Path(\"RESULTS/ROBUST_THRESHOLD\")\n",
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ROBUST_THRESHOLD_GRID = {\n",
        "    \"IQR\": [0.75, 1.0, 1.5, 2.0, 2.5],\n",
        "    \"MAD\": [2.0, 2.5, 3.0, 3.5, 4.0],\n",
        "    \"Z_SCORE_BUNDLE\": [1.5, 2.0, 2.5, 3.0, 3.5],\n",
        "    \"Z_SCORE_METRIC\": [1.5, 2.0, 2.5, 3.0],\n",
        "    \"SN\": [2.0, 2.5, 3.0, 3.5],\n",
        "    \"QN\": [2.0, 2.5, 3.0, 3.5],\n",
        "}\n",
        "\n",
        "METRICS = get_metrics()\n",
        "CAMCAN_REF = {metric: get_camcan_file(metric) for metric in METRICS}\n",
        "\n",
        "# Ajustez les filtres pour limiter la charge (None => tous disponibles)\n",
        "SELECTED_SAMPLE_SIZES = {100}   # mettre None pour tous\n",
        "SELECTED_RATIOS = None          # exemple: {3, 10, 30}\n",
        "SELECTED_TEST_IDS = set(range(3))  # mettre None pour utiliser les 20 tests\n",
        "USE_RWP = False\n",
        "HC_ONLY = False\n",
        "LOAD_EXISTING = False\n",
        "\n",
        "\n",
        "def _to_optional_set(values):\n",
        "    if values is None:\n",
        "        return None\n",
        "    if isinstance(values, set):\n",
        "        return values\n",
        "    return set(values)\n",
        "\n",
        "\n",
        "SELECTED_SAMPLE_SIZES = _to_optional_set(SELECTED_SAMPLE_SIZES)\n",
        "SELECTED_RATIOS = _to_optional_set(SELECTED_RATIOS)\n",
        "SELECTED_TEST_IDS = _to_optional_set(SELECTED_TEST_IDS)\n",
        "\n",
        "print(f\"Méthodes configurées : {', '.join(ROBUST_THRESHOLD_GRID)}\")\n",
        "print(f\"Métriques : {', '.join(METRICS)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iter_cases(processed_root, synthetic_root, sample_filter=None, ratio_filter=None, test_filter=None):\n",
        "    cases = []\n",
        "    for sr_dir in sorted(processed_root.iterdir()):\n",
        "        if not sr_dir.is_dir():\n",
        "            continue\n",
        "        try:\n",
        "            sample_str, ratio_str = sr_dir.name.split(\"_\")\n",
        "            sample_size = int(sample_str)\n",
        "            ratio_percent = int(ratio_str)\n",
        "        except ValueError:\n",
        "            continue\n",
        "        if sample_filter and sample_size not in sample_filter:\n",
        "            continue\n",
        "        if ratio_filter and ratio_percent not in ratio_filter:\n",
        "            continue\n",
        "        synthetic_sr_dir = synthetic_root / sr_dir.name\n",
        "        if not synthetic_sr_dir.exists():\n",
        "            continue\n",
        "        test_dirs = [d for d in sr_dir.iterdir() if d.is_dir() and d.name.isdigit()]\n",
        "        for test_dir in sorted(test_dirs, key=lambda p: int(p.name)):\n",
        "            test_idx = int(test_dir.name)\n",
        "            if test_filter and test_idx not in test_filter:\n",
        "                continue\n",
        "            synthetic_test_dir = synthetic_sr_dir / test_dir.name\n",
        "            if not synthetic_test_dir.exists():\n",
        "                continue\n",
        "            cases.append({\n",
        "                \"sample_size\": sample_size,\n",
        "                \"ratio\": ratio_percent / 100.0,\n",
        "                \"ratio_percent\": ratio_percent,\n",
        "                \"label\": sr_dir.name,\n",
        "                \"test_index\": test_idx,\n",
        "                \"processed_dir\": test_dir,\n",
        "                \"synthetic_dir\": synthetic_test_dir,\n",
        "            })\n",
        "    return cases\n",
        "\n",
        "\n",
        "def evaluate_method_thresholds(method, thresholds, cases, metrics):\n",
        "    metric_records = []\n",
        "    bundle_records = []\n",
        "    for threshold in thresholds:\n",
        "        robust_params = {\"threshold\": threshold}\n",
        "        total_steps = len(cases) * len(metrics)\n",
        "        if total_steps == 0:\n",
        "            continue\n",
        "        with tqdm(total=total_steps, desc=f\"{method} | thr={threshold}\") as pbar:\n",
        "            for case in cases:\n",
        "                sample_size = case[\"sample_size\"]\n",
        "                ratio = case[\"ratio\"]\n",
        "                ratio_percent = case[\"ratio_percent\"]\n",
        "                label = case[\"label\"]\n",
        "                test_idx = case[\"test_index\"]\n",
        "                processed_dir = case[\"processed_dir\"]\n",
        "                synthetic_dir = case[\"synthetic_dir\"]\n",
        "                for metric in metrics:\n",
        "                    metric_dir = processed_dir / metric\n",
        "                    train_file = metric_dir / f\"train_{label}_{test_idx}_{metric}.csv\"\n",
        "                    test_file = metric_dir / f\"test_{label}_{test_idx}_{metric}.csv\"\n",
        "                    if not train_file.exists() or not test_file.exists():\n",
        "                        pbar.update(1)\n",
        "                        continue\n",
        "                    gt_test_file = synthetic_dir / f\"gt_test_{label}_{test_idx}_{metric}.csv\"\n",
        "                    if not gt_test_file.exists():\n",
        "                        pbar.update(1)\n",
        "                        continue\n",
        "                    run_dir = OUTPUT_ROOT / method / f\"thr_{format_param_token(threshold)}\" / label / str(test_idx) / metric\n",
        "                    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    model_path = fit(\n",
        "                        str(train_file),\n",
        "                        CAMCAN_REF[metric],\n",
        "                        metric,\n",
        "                        HARMONIZATION_METHOD,\n",
        "                        method,\n",
        "                        USE_RWP,\n",
        "                        str(run_dir),\n",
        "                        HC_ONLY,\n",
        "                        robust_params=robust_params,\n",
        "                    )\n",
        "                    test_output = apply(\n",
        "                        str(test_file),\n",
        "                        model_path,\n",
        "                        metric,\n",
        "                        HARMONIZATION_METHOD,\n",
        "                        method,\n",
        "                        USE_RWP,\n",
        "                        str(run_dir),\n",
        "                        robust_params=robust_params,\n",
        "                    )\n",
        "                    mae_df = compare_with_compilation(\n",
        "                        pd.read_csv(test_output),\n",
        "                        pd.read_csv(gt_test_file),\n",
        "                    )\n",
        "                    if mae_df.empty:\n",
        "                        pbar.update(1)\n",
        "                        continue\n",
        "                    row = mae_df.iloc[0]\n",
        "                    mean_mae = float(row.mean())\n",
        "                    metric_records.append({\n",
        "                        \"method\": method,\n",
        "                        \"threshold\": threshold,\n",
        "                        \"sample_size\": sample_size,\n",
        "                        \"disease_ratio\": ratio,\n",
        "                        \"ratio_percent\": ratio_percent,\n",
        "                        \"test_index\": test_idx,\n",
        "                        \"metric\": metric,\n",
        "                        \"mae\": mean_mae,\n",
        "                    })\n",
        "                    for bundle_name, value in row.items():\n",
        "                        bundle_records.append({\n",
        "                            \"method\": method,\n",
        "                            \"threshold\": threshold,\n",
        "                            \"sample_size\": sample_size,\n",
        "                            \"disease_ratio\": ratio,\n",
        "                            \"ratio_percent\": ratio_percent,\n",
        "                            \"test_index\": test_idx,\n",
        "                            \"metric\": metric,\n",
        "                            \"bundle\": bundle_name,\n",
        "                            \"mae\": float(value),\n",
        "                        })\n",
        "                    pbar.update(1)\n",
        "    return (\n",
        "        pd.DataFrame(metric_records),\n",
        "        pd.DataFrame(bundle_records),\n",
        "    )\n",
        "\n",
        "\n",
        "def run_all_methods():\n",
        "    cases = iter_cases(\n",
        "        PROCESSED_ROOT,\n",
        "        SYNTHETIC_ROOT,\n",
        "        sample_filter=SELECTED_SAMPLE_SIZES,\n",
        "        ratio_filter=SELECTED_RATIOS,\n",
        "        test_filter=SELECTED_TEST_IDS,\n",
        "    )\n",
        "    print(f\"{len(cases)} cas sélectionnés.\")\n",
        "    metric_frames = []\n",
        "    bundle_frames = []\n",
        "    for method, thresholds in ROBUST_THRESHOLD_GRID.items():\n",
        "        if not thresholds:\n",
        "            continue\n",
        "        metric_df, bundle_df = evaluate_method_thresholds(method, thresholds, cases, METRICS)\n",
        "        if not metric_df.empty:\n",
        "            metric_frames.append(metric_df)\n",
        "        if not bundle_df.empty:\n",
        "            bundle_frames.append(bundle_df)\n",
        "    metric_results = pd.concat(metric_frames, ignore_index=True) if metric_frames else pd.DataFrame()\n",
        "    bundle_results = pd.concat(bundle_frames, ignore_index=True) if bundle_frames else pd.DataFrame()\n",
        "    return metric_results, bundle_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summaries_dir = OUTPUT_ROOT / \"summaries\"\n",
        "summaries_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if LOAD_EXISTING and (summaries_dir / \"metric_results.csv\").exists():\n",
        "    metric_results = pd.read_csv(summaries_dir / \"metric_results.csv\")\n",
        "    bundle_results = pd.read_csv(summaries_dir / \"bundle_results.csv\")\n",
        "    print(\"Résultats chargés depuis les fichiers existants.\")\n",
        "else:\n",
        "    metric_results, bundle_results = run_all_methods()\n",
        "    if not metric_results.empty:\n",
        "        metric_results.to_csv(summaries_dir / \"metric_results.csv\", index=False)\n",
        "    if not bundle_results.empty:\n",
        "        bundle_results.to_csv(summaries_dir / \"bundle_results.csv\", index=False)\n",
        "    print(\"Évaluation terminée.\")\n",
        "\n",
        "metric_results.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if metric_results.empty:\n",
        "    raise ValueError(\"Aucun résultat MAE n'a été généré. Vérifiez les filtres ou relancez l'évaluation.\")\n",
        "\n",
        "metric_summary = (\n",
        "    metric_results\n",
        "    .groupby([\"method\", \"threshold\", \"metric\"], as_index=False)\n",
        "    .agg(\n",
        "        mean_mae=(\"mae\", \"mean\"),\n",
        "        std_mae=(\"mae\", \"std\"),\n",
        "        n_cases=(\"mae\", \"count\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "global_summary = (\n",
        "    metric_results\n",
        "    .groupby([\"method\", \"threshold\"], as_index=False)\n",
        "    .agg(\n",
        "        mean_mae=(\"mae\", \"mean\"),\n",
        "        std_mae=(\"mae\", \"std\"),\n",
        "        n_cases=(\"mae\", \"count\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "metric_summary.to_csv(summaries_dir / \"metric_summary.csv\", index=False)\n",
        "global_summary.to_csv(summaries_dir / \"global_summary.csv\", index=False)\n",
        "\n",
        "metric_summary.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "plots_dir = OUTPUT_ROOT / \"plots\"\n",
        "plots_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for method in ROBUST_THRESHOLD_GRID:\n",
        "    method_metric = metric_summary[metric_summary[\"method\"] == method]\n",
        "    if method_metric.empty:\n",
        "        continue\n",
        "    method_dir = plots_dir / method\n",
        "    method_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for metric in sorted(method_metric[\"metric\"].unique()):\n",
        "        data = method_metric[method_metric[\"metric\"] == metric].sort_values(\"threshold\")\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        ax.errorbar(\n",
        "            data[\"threshold\"],\n",
        "            data[\"mean_mae\"],\n",
        "            yerr=data[\"std_mae\"].fillna(0.0),\n",
        "            fmt=\"-o\",\n",
        "            capsize=4,\n",
        "            linewidth=2,\n",
        "        )\n",
        "        ax.set_xlabel(\"Seuil\")\n",
        "        ax.set_ylabel(\"MAE moyen\")\n",
        "        ax.set_title(f\"{method} – {metric.upper()}\")\n",
        "        ax.set_xticks(data[\"threshold\"])\n",
        "        ax.grid(True, axis=\"y\", alpha=0.3)\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(method_dir / f\"{metric}_threshold_sweep.png\", dpi=200)\n",
        "        plt.close(fig)\n",
        "\n",
        "    data_global = global_summary[global_summary[\"method\"] == method].sort_values(\"threshold\")\n",
        "    if data_global.empty:\n",
        "        continue\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.errorbar(\n",
        "        data_global[\"threshold\"],\n",
        "        data_global[\"mean_mae\"],\n",
        "        yerr=data_global[\"std_mae\"].fillna(0.0),\n",
        "        fmt=\"-o\",\n",
        "        capsize=4,\n",
        "        linewidth=2,\n",
        "    )\n",
        "    ax.set_xlabel(\"Seuil\")\n",
        "    ax.set_ylabel(\"MAE moyen (toutes métriques)\")\n",
        "    ax.set_title(f\"{method} – Moyenne globale\")\n",
        "    ax.set_xticks(data_global[\"threshold\"])\n",
        "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(method_dir / \"global_threshold_sweep.png\", dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "plots_dir\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}